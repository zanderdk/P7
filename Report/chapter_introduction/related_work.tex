\section{Related Work}\label{sec:related_work}
The problem of linking articles on Wikipedia, i.e.\ the linking problem, has been the subject of previous research, and so there is much material to gain inspiration from. However, before we draw inspiration from solutions to related problems, we must first consider how to approach our problem \todo{fix argumentation}. As with almost all problems, this one can be perceived from multiple angles and modeled in many different ways. Here we will consider some general approaches and examine how they previously have been used for analyzing relationships between hypertext documents such as Wikipedia articles.
\todo{Læg op til de kommende 3 approaches}
%\todo{Omskriv sidste 3 sætninger. Vi ved godt hvad problemet er her, vi kigger på relateret arbejde for at få ideer til løsningsmuligheder.}

\subsection{Content-Based Analysis}\label{related_semantic_contextual}
The linking problem can be seen as a problem of deducing content-based relations. Solutions to this problem usually involve a degree of textual analysis, in order to determine whether a piece of text refers to a subject. The problem is usually divided into two parts, each given a different level of importance depending on the solution:

\begin{description}
  \item[Syntactic Recognition] This part finds syntactic references to some subject. One approach is to find keywords or shingles using an n-gram technique and matching those to subjects~\cite{mihalcea2007wikify}.

  \item[Semantic Recognition] This part combats possible syntactic ambiguity, for example determining whether the word \emph{tree} semantically refers to the data structure or the plant. One of many approaches to semantic recognition is classification, as seen in~\cite{milne2008learning}. Here they attempt to determine semantic references by training on metrics for commonness, relatedness, and context quality. In short, commonness is the probability distribution of references, relatedness is a measure of similarity between the referrer and the possibly referenced subject, and context quality is a measure of whether a given term is usually linked. The example given in~\cite{milne2008learning} is the English grammatical article \enquote{the}, which is used often, but rarely links to the subject article.
\end{description}

Machine learning is often used in related work that takes a content-based approach. In~\cite{mihalcea2007wikify,milne2008learning} a naive Bayes classifier is employed, with the latter also testing support vector machines and different decision trees, for the purpose of classifying matters of ambiguity and relatedness.

\subsection{Structure-Based Analysis}\label{related_structural_analysis}
Another way of viewing the linking problem is as missing structural connections in a dataset. With this approach, solutions attempt to analyze a structure in order to discover possible patterns that influences linking. For Wikipedia a relevant structure to consider is a graph structure with articles as nodes. There are multiple ways to model edges in the graph. In~\cite{hyperlink-structure-using-logs} edges are created from the navigation of Wikipedia's visitors, based on the belief that an optimal linking structure can be deduced from user behavior. Essentially, anything that can be considered a relation between any two articles, can be the basis for a set of edges, as explored in~\cite{lu2011link}.

Regardless of the approach, a metric for an appropriate link must be considered. We have already introduced Wikipedia's guidelines on linking~\cite{wiki-editor-guidelines}, which is Wikipedia's official view of such a metric. However, even though they clearly hold authority on the matter, there exists alternatives worth considering.

In~\cite{hyperlink-structure-using-logs} they consider appropriate links to be ones that are in use by visitors. Their technique seeks to approximate the number of clicks a potential missing link would receive out of the total visitors on the article given its inclusion. This is called the \emph{clickthrough rate} and is based on the idea that the most useful links get the most clicks, and that links are only useful if they are clicked. An argument in favor of using clickthrough rate is that every link can be objectively measured and compared.

%One has to be aware of the competing links problem, where too many links are inserted on a popular article. This is solved in \cite{hyperlink-structure-using-logs} by optimizing an objective function with a diminishing returns property such that new links will naturally be spread on a larger number of articles.

%\subsection{Machine Learning}\label{related_machine_learning}



%\cite{hyperlink-structure-using-logs} models user behavior as objective functions. The best performing objective functions are designed to have diminishing returns such that new links are spread evenly across Wikipedia. To return a list of link suggestions, they propose a greedy algorithm that seeks to maximize the designed objective functions, ultimately optimizing for the expected number of clicks a new link will receive.

%Wikify! Linking Documents to Encyclopedic Knowledge 		mihalcea2007wikify
%Learning to Link with Wikipedia 							milne2008learning
%Hyperlink Structure Using Server logs						hyperlink-structure-using-logs
%Human Navigation Traces									west2015mining
%Prediction in Complex Networks 							lu2011link
%Link Prediction using Supervised Learning 					al2006link
%LINE algo													tang2015line
