%\section{Lessons Learned}
%\todo{Title seems funny. On the other hand ``Lessons Learned'' is a completely valid term according to SWEBOK\@.}

During this project we have gained valuable experiences in many areas such as project management, working with large datasets, feature engineering, feature learning, and machine learning. This section will reflect on these areas.

\section{Problem Definition}
The problem we have worked with in this project touches many different subjects. This broadness allowed us to examine different possible project directions, and align the solution with both prior interests and newly acquired knowledge. Conversely, having more possibilities forced us to distribute the time spent on evaluating options, causing each to become less thoroughly researched. 

If we had delimited the problem early in the project, we could have focused on a more narrow field of research. It is however hard to conclude whether time gained from early delimitations would have been spent on solving the problem in a more convoluted and restricted way.

%Another approach we have discussed was to use the Wikipedia clickstream as an indication of which links users are using frequently and from the referrer which articles are being navigated between even when a link is not present. This might have been used to determine missing links and would have increased the amount of available training data, but also imposes the problem that we cannot be sure that the links are actually good or missing \todo{have we described this somewhere}. For this reason we instead used the clickstream as a heuristic to generate pairs of articles as described in \cref{sec:candidate_clickstream}.

\section{Handling Large Datasets}
%We found that we spent a significant portion of the project time on preparing data for the database. Initially we did not know the exact direction we wanted to take regarding the solution, so we stored all data that seemed relevant in the database. This was both slow and cumbersome. As we further narrowed the solution space, it became obvious what data was relevant to store. Preparing and finding a sensible data representation takes a certain amount of time that has to be considered when planning a machine learning project.
In the early stages of the project development, multiple directions were considered. Because of this, we initially worked with a database with all seemingly relevant data stored. This caused database operations to become slow and cumbersome. If we had spent more time on delimiting the problem, we could have decided which data we wanted to learn features from, which would have decreased the time spent managing the database. Alternatively, we could have taken advantage of the fact that the Neo4j database is schema-less, and started with a minimal representation and added data when a need for it appeared.

When working with large datasets, experimentation is hindered by long processing times. This means that we had to thoroughly consider the return of investment of every choice we made. In general, we found machine learning to be a process of trial and error. The combination of having long processing times and doing a project which focused on learning through experience, caused a considerable amount of time to be spent on experiments that did not move the project forward. Despite this we gained a better understanding of the field of machine learning during the project.

\section{Feature Generation}
We started the project experimenting with manually engineered features. Roughly a month was spent on this but the results were not encouraging and we found it difficult to predict which features were good. Therefore we explored feature learning as an alternative way to find good features.

Our inexperience with feature learning caused us to repeat some tests and experiments, since some errors only became apparent to us later in the process. This was especially relevant for the parameter optimization, as the turnaround time was relatively long.

We also realized, that using a single classifier's performance as the only measure for the parameter optimizing component, and expecting it to generalize across all classifiers, is a naive approach. Ideally we should have used a range of classifiers, and added their individual parameters to the set of hyperparameters we were optimizing. However since this would significantly increase the turnaround time for the optimization, it is not a viable solution. Alternatively, we could have worked on evaluating the parameters of the feature learning algorithms, not by testing the features on a classifier, but instead by directly checking the output feature vectors. Finding a way of reliably doing such a check would however have been a difficult task, that had required much work, but would have been interesting.

\section{Data Propagation}
Each of the system's components interprets and propagates data. Since this data is never tested between components, any error made in a previous component, will carry over to the next. This made it hard to diagnose the cause of any given problem, especially in the later stages of the pipeline. In order to mitigate the effect of this error propagation, we could have been more thorough during our considerations of the information that was available in the data, and whether it was reasonable for the classifier to find results within it.

It could also have been worth spending more time trying to develop test methods for the data. We refrained from this during the project, believing that the only way to truly test data, was to see if the classifier could find results, and that we should not discard data just because our own reasoning could not find it useful. While there might be some truth to this, it would have been possible to develop tests that would at least have raised some concerns earlier.


%Points:
%- to minimize the problem, we could have narrowed the problem from the start, eg. a subset of wiki, choose a specific approach faster
%    - Would allow us to iterate faster
%    - Hard to generalize to the whole of wikipedia
%    - would narrow the solution space
%    - We could have had more focus on exploring approaches rather than implementing a final solution with a narrow approach set
%- dealing with large data needs special considerations
%    - takes time to try new things/approaches
%    - Scheduling/planning: Try to always keep a worker machine busy

%- It is hard to reason the effects of choices made before testing them. Experience in machine learning is a thing! Good that we have now had experience with it
%- Finding good features is vital
%- It can be hard to predict whether a solution will generalize before trying it
%- As machine learning is about experience, there is a lot of trial and error
%- Preparing data and finding the optimal data representation in eg. database takes time
%- feature learning: because we lack experience designing features, feature learning seems like an alternative that can yield good results even by inexperienced developers.
%    - Feature engineering is expensive
%- parameter optimization
%    - isolate components being optimized
%- plan ahead:
%    - for example in node2vec, we optimized towards f1, when precision would probably make more sense.
%    - define ahead what we want to optimize. Which use cases?