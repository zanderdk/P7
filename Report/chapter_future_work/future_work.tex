\chapter{Future Work}\label{sec:future_work}
During the project we encountered different areas that could be suited for future development. In this chapter we present three main areas that would be interesting to consider for future development of the system. 

\section{Improving Data Quality}
Data extracted from the \emph{Database} component is used extensively throughout the system, particularly for feature learning and classification. This makes the quality of data stored in the database a major concern. As mentioned in \cref{sec:db_populate}, we currently store \num{11159213} articles in the database. The majority of these articles are non-encyclopedia articles, that provides little or no content. In \cref{sec:classification_evaluation} we identify these non-encyclopedia articles as being potentially detrimental to the performance of both the \emph{Classifier} and the \emph{Feature Extractor} components. The \emph{Feature Extractor} is affected by the non-encyclopedia articles as the walks have to traverse many irrelevant nodes in the graph, and these nodes often have no outgoing edges, causing the walks to end prematurely. The \emph{Classifier} is affected as the negative training pairs are sampled randomly from the entire distribution, resulting in a majority of negative pairs containing non-encyclopedia articles. Therefore, improving the data quality, by simply pruning non-encyclopedia articles from the dataset, will likely improve both the feature model and the classification model.

Even in a dataset containing only encyclopedia articles, the random sampling of negative training pairs is problematic, as the negative pairs contain very few cases where articles have high relevance, but still should not be linked. Additionally, the training data only contains pairs with a feature source article, leading to generality problems when attempting classification of non-featured article pairs. Therefore, the quality of the training pairs could be further improved by using a different sampling approach, though this would also require an approach to label non-featured article pairs.

We currently only consider Wikipedia data from 2015. A future improvement would be to consider the live version of Wikipedia. By working on real-time Wikipedia data, the system could be improved to suggest links for recently edited and added articles. This would be beneficial for editors, since new articles are unlikely to be appropriately linked.

\section{User Interface}
The are several different aspects of the user interface that could be improved. The API currently offer limited functionality, supporting just a few primary use cases. This could be improved to provide more specific services. A usability analysis of the current implementation could be performed, to uncover additional requirements to the API functionality. Based on our assumptions of the API usage, we believe the following two API endpoints could be useful:

\begin{enumerate}
    \item Given articles \emph{a} and \emph{b} as parameters, returns whether \emph{a} should link to \emph{b}.
    \item Given article \emph{a}, returns a list of articles that \emph{a} should link to.
\end{enumerate}

The current system only provides suggestions on whether or not two articles should be linked. While this can assist the users, they still need to review the link suggestion, and determine where it could potentially be added. To further assist the users, and speed up the review process, the system could suggest where in the articles text the link should be placed.

\section{Different Feature Approaches}
In the graph model of Wikipedia, we currently only model links between articles as edges, though it is possible to include any kind of relative information between two articles. By modeling additional information as edges or weights, it could be possible for the \emph{Feature Extractor} component to learn different features using these different edges. A possible method would be to learn different models for each set of edges, and then combining the individual feature vectors they produce.

Alternative sources of information that could be used, either as a supplement or a substitute, could be content-based similarities. Examples of this could be doc2vec~\cite{le2014distributed}, or perhaps the term similarity feature we introduce in \cref{sec:feature_engineering}. In fact, any feature that can be extracted from an article can be combined. This reintroduces feature engineering, since we would now have to engineer which learned feature vectors we wish to use.

%Modeling of content-based similarities, could allow content information in the structural feature learning process. It would be interesting to explore possibilities of including additional information as edges in the graph, in order to improve the generated feature representations.
%\todo{Der skal nok st√• noget mere her}
% categories + text analysis
%Some other relevant information to include could be article categories, or information gained from a textual analysis of the articles. 

% reinforcement learning
%The current UI is designed to accept feedback on the suggested links, but the feedback is currently not used. Reinforcement learning techniques could be used to improve the performance of the system by learning on the given user feedback.

% candidate generator
%To be able to scale the system for more users, an adequate number of link suggestions has to be available for users to consider. One way to increase the number of suggestions is by improving the quality of the candidate pairs generated by the candidate generator. It would be useful to examine methods for increasing the likelihood that relevant candidates are generated, while still keeping the amount of irrelevant candidates down to a manageable level. A possible solution would be to supplement or replace the existing clickstream and n-gram approach with one that ranks all article pairs based on similarity. The similarity score could be derived from the underlying word2vec model in node2vec.

% opposite direction
%Additionally, it might be useful to extend the training data with training pairs for linking in the opposite direction, since we do not know to which extend the direction between pairs of articles is being considered. 

% number of samples
%Furthermore, the number of samples in the training data could be increased to allow the classifier to generalize better on the whole of Wikipedia. One way to increase the amount of training data would be to also consider links from good articles as positives, even though we cannot assume them to have appropriate linking.
