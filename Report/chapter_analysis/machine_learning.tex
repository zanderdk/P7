\section{Machine Learning Task}\label{sec:machine_learning_task}
%With a graph defined,
In this section we introduce a machine learning task based on the problem we are trying to solve. In \cref{sec:problem_statement} we describe the problem of suggesting links between Wikipedia articles. To solve this using machine learning we formulate it as a binary classification problem of determining whether two Wikipedia articles should be linked or not.
%Specifically deciding between the classes \textit{linked} and \text{not linked}.

For the reasons described in \cref{sec:choice_of_graph} we assume featured articles to have appropriate linking. This makes links from featured articles suitable as positive training examples when learning where a link should be present. Similarly articles that are not linked from a featured article can be used as negative training examples.
Under this assumption a large amount of labeled training data can be generated and we therefore limit the scope of the problem to only consider supervised learning approaches. This delimits the problem to binary classification problem in a supervised machine learning context. \todo{should we explain why labeled training data makes supervised learning a good fit?}


%By constructing the training data only from $P$ and $N$, all training pairs can be labeled as either linked or not linked. 

\subsection{Definitions}
%We define a set of positive labeled training pairs $P \subset V \times V$ as $\Set{(a,b)\ |\ a\text{ is featured}\ \wedge\ a \Rightarrow b}$. Likewise we define a set of negative labeled training pairs $N \subset V \times V$ as $\set{(a,b)\ |\ a\text{ is featured}\ \wedge\ a \not\Rightarrow b}$.
We define a set of positive labeled training pairs $P \subset V \times V$, as well as a set of negative labeled training pairs $N \subset V \times V$:

$$P = \Set{(a,b)\ |\ a\text{ is featured}\ \wedge\ a \Rightarrow b},$$
$$N = \Set{(a,b)\ |\ a\text{ is featured}\ \wedge\ a \not\Rightarrow b}$$

%\subsubsection{Binary Classification}

%The problem can be defined as a binary classification:
We formulate the classification task as an optimization problem, maximizing the probability of $(a,b) \in \ \Rightarrow$. Let $f: V\times V \to \mathbb{R}^d$ be the function mapping from article pairs to a vector representation of features where $d$ is the dimension of the vector.

In order to learn predicting the linking class label $y$ of a pair of articles $x$,
%we seek to optimize the following objective function, by maximizing the probability of $x \in \ \Rightarrow$ conditioned on the feature representation $f(x)$, where $x \in V \times V$:
we seek to find values of $y$ that maximizes the objective function $\Pr(y \mid f(x))$, which represents the probability that articles should be linked, where $x \in \ \Rightarrow$ conditioned on the feature representation $f(x)$, and $x \in V \times V$:

\begin{equation}
\label{objective}
\argmax_y\ \Pr(y \mid f(x))
\end{equation}

With $y$ defined as:

\[
    y=
\begin{cases}
    1 & \text{if } x \in \  \Rightarrow\\
    0 & \text{otherwise}
\end{cases}
\]

Here, 1 represents the class of article pairs that should be linked, and 0 the ones that should not.
%$\Pr$ in \cref{objective} represents the probability that a pair of articles should be linked, and is the function we aim to learn.
In order to solve this optimization problem we first need to define $f$ which will be described in the following section.

\subsubsection{Article Features}
\todo{this needs review, and should maybe be moved to another section}

The function $f: V\times V \to \mathbb{R}^d$ maps article pairs to a feature representation in $d$ dimensions, by combining the features of each article pair.

f(a,b) = g(h(a), h(b)) where $h: V \to \mathbb{R}^d$ is a function mapping an article to a feature vector and $g: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^d$ a binary operation combining a pair of feature vectors.

The feature vector $h(x)$ represents the learned features of article $x$.

%\paragraph{Input:}
%A set of $m$ training examples $(x^j,y^j)$ for $j=1,2..m$, sampled from a distribution $f(E_j)$ of article pair features, with $x^j \in R^n$ being a feature vector and $y^j \in \set{-1,+1}$ a training label. A training label $y^j=+1$ will denote a \textit{positive sample} and $y^j=-1$ a \textit{negative sample}. The i-th feature of a sample $x^j$ defined $x^j_i$ represents a pair of articles and is a combination of article features generated by node2vec~\cite{node2vec} from the training pairs $P$ and $N$.

%\todo{explain/generalize article combination without mentioning node2vec? (binary operation)}
%\todo{beskriv hvilken objective function vi vil minimere/maksimere}
%\todo{input: $x^j$ skal v√¶re par af artikler}

%node2vec:
%Given two nodes u and v, and a binary operator o' the feature vectors f(u) and f(v) are combined into the representation g(u,v).

%\paragraph{Output:}
%A function $f: R^n \to \set{-1,+1}$ that classifies additional samples ${x^k}$ sampled from $F$.



