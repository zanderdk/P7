%\section{Machine Learning}
%In this chapter we will describe the diffrent machine learning approches that has been taken into consideration and the reasoning behind.


\section{Machine Learning Task}\label{sec:machine_learning_task}
Before choosing which specific machine learning algorithms to use, we first consider which machine learning task would fit this project based on the problem we are trying to solve. In \cref{sec:problem_statement} we describe the problem of suggesting links between Wikipedia articles, part of which is to determine whether two Wikipedia articles should be linked. This problem can be seen as a binary classification problem.

As described in \cref{ch:introduction} there are $5000$ featured Wikipedia articles, which we assume to have appropriate linking. Under this assumption we can define a set of positive labeled training pairs $P$ as $\set{(a,b)\ |\ a\text{ is featured}\ \wedge\ a \to b}$ where $a \to b$ denotes that article $a$ has a link to article $b$. Likewise we define a set of negative labeled training pairs $N$ as  $\set{(a,b)\ |\ a\text{ is featured}\ \wedge\ a \not\to b}$, where $a \not\to b$ denotes article $a$ not having a link to article $b$.

By constructing the training data only from $P$ and $N$, all training pairs can be labeled as either linked or not linked. This further delimits the problem to be a supervised binary classification problem.
 
\subsubsection{Binary Classification}

The problem can be defined as a binary classification:

\paragraph{Input:}
A set of $m$ training examples $(x^j,y^j)$ for $j=1,2..m$, sampled from a distribution $f(E_j)$ of article pair features, with $x^j \in R^n$ being a feature vector and $y^j \in \set{-1,+1}$ a training label. A training label $y^j=+1$ will denote a \textit{positive sample} and $y^j=-1$ a \textit{negative sample}. The i-th feature of a sample $x^j$ defined $x^j_i$ represents a pair of articles and is a combination of article features generated by node2vec~\cite{node2vec} from the training pairs $P$ and $N$.

\todo{explain/generalize article combination without mentioning node2vec? (binary operation)}
\todo{beskriv hvilken objective function vi vil minimere/maksimere}
\todo{input: $x^j$ skal v√¶re par af artikler}

%node2vec:
%Given two nodes u and v, and a binary operator o' the feature vectors f(u) and f(v) are combined into the representation g(u,v).

\paragraph{Output:}
A function $f: R^n \to \set{-1,+1}$ that classifies additional samples ${x^k}$ sampled from $F$.



\section{new stuff}


\begin{itemize}
\item $G = (V, E)$: graph
\item $V$: set of all Wikipedia articles
\item $F \subset V$: set of all article pairs
\item $E \subseteq F \times V$: set of links from featured articles
\item $g$: objective function...
%\item $h: V \times V \to R^d$: feature function*
\item $h: E \to R^d$: feature function*
%\item $f: R^d \times \set{-1,1} \to \mathbb{R}$: 
\end{itemize}

%$P \subset E$

The graph $G$ represents articles and link structure.

%$G = (V, E)$ where $V$ is the set of all Wikipedia articles, and E is the set of links from featured articles. $E \subseteq F \times V$.

\subsection{more new stuff}

$h: V \times V \in R^d$ % feature of

$g(x) = arg max_y f(h(x), y)$

$g(x) = arg max_y Pr(y | h(x))$

where $f: R^d \times \set{-1,1} \to \mathbb{R}$, and $h: V \times V \to R^d$

$y \in \set{-1,1}$
%$y = \set{1 if x \in E, -1 if x \in E}$

\begin{equation}
  y^j =
  \begin{cases}
    1, & \text{if}\ j \in E \\
    -1, & \text{otherwise}
  \end{cases}
\end{equation}

