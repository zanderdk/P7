%\section{Machine Learning}
%In this chapter we will describe the diffrent machine learning approches that has been taken into consideration and the reasoning behind.


\section{Machine Learning Task}\label{sec:machine_learning_task}
Before choosing which specific machine learning algorithms to use, we first consider which machine learning task would fit this project based on the problem we are trying to solve. In \cref{sec:problem_statement} we describe the problem of suggesting links between Wikipedia articles, part of which is to determine whether two Wikipedia articles should be linked. This problem can be seen as a binary classification problem.

As described in \cref{ch:introduction} there are $5000$ featured Wikipedia articles, which we assume to have appropriate linking. Under this assumption we can define a set of positive labeled training pairs $P \subset V \times V$ as $\set{(a,b)\ |\ a\text{ is featured}\ \wedge\ a \Rightarrow b}$. Likewise we define a set of negative labeled training pairs $N \subset V \times V$ as $\set{(a,b)\ |\ a\text{ is featured}\ \wedge\ a \not\Rightarrow b}$
By constructing the training data only from $P$ and $N$, all training pairs can be labeled as either linked or not linked. \todo{forklar at vi ønsker suporvised learning da det er nemmer end semi suporvised og PU hint find kilde} This further delimits the problem to be a supervised binary classification problem.
 
\subsubsection{Binary Classification}

%The problem can be defined as a binary classification:
We fomulate this clasification task as a optimization problem, maximizing the probability of $(a,b) \in \ \Rightarrow$. Let $f: V\times V \to R^d$ be the function mapping from article pairs to a feature representation where $d$ is the dimension of the feature representation.
We seek to optimize the following objective function, which maximizes the probability of $x \in \ \Rightarrow$ conditioned on the feature representaion $f(x)$, where $x \in V \times V$:

\begin{equation}
\label{objective}
\argmax_y\ Pr(y | f(x))
\end{equation}

With $y$ defined as:

\[
    y= 
\begin{cases}
    1 & \text{if } x \in \  \Rightarrow\\
    0 & \text{otherwise}
\end{cases}
\]

1 represents the class of linked article pairs $\Rightarrow$, and 0 the article pair that are not linked $\neg \Rightarrow$. $Pr$ in \cref{objective} is the function we aim to learn. In order to solve this optimization problem we first need to define $f$ which will be described in the following section.

 
%\paragraph{Input:}
%A set of $m$ training examples $(x^j,y^j)$ for $j=1,2..m$, sampled from a distribution $f(E_j)$ of article pair features, with $x^j \in R^n$ being a feature vector and $y^j \in \set{-1,+1}$ a training label. A training label $y^j=+1$ will denote a \textit{positive sample} and $y^j=-1$ a \textit{negative sample}. The i-th feature of a sample $x^j$ defined $x^j_i$ represents a pair of articles and is a combination of article features generated by node2vec~\cite{node2vec} from the training pairs $P$ and $N$.

%\todo{explain/generalize article combination without mentioning node2vec? (binary operation)}
%\todo{beskriv hvilken objective function vi vil minimere/maksimere}
%\todo{input: $x^j$ skal være par af artikler}

%node2vec:
%Given two nodes u and v, and a binary operator o' the feature vectors f(u) and f(v) are combined into the representation g(u,v).

%\paragraph{Output:}
%A function $f: R^n \to \set{-1,+1}$ that classifies additional samples ${x^k}$ sampled from $F$.



