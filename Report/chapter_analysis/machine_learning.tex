\section{Machine Learning Task}\label{sec:machine_learning_task}
%With a graph defined,
In this section we introduce a machine learning task based on the problem we are trying to solve. In \cref{sec:problem_statement} we describe the problem of suggesting links between Wikipedia articles. To solve this using machine learning we formulate it as a binary classification problem of determining whether two Wikipedia articles should be linked.
%Specifically deciding between the classes \textit{linked} and \text{not linked}.

For the reasons described in \cref{sec:choice_of_graph} we assume featured articles to have appropriate linking. This makes links from featured articles suitable as positive training examples when learning where a link should be present. Similarly articles that are not linked from a featured article can be used as negative training examples.

We define the set of positive training pairs $P \subset V \times V$, as well as a set of negative training pairs $N \subset V \times V$:

\begin{align*}
P &= \Set{(a,b) \mid a\text{ is featured}\ \wedge\ a \Rightarrow b}, \\
N &= \Set{(a,b) \mid a\text{ is featured}\ \wedge\ a \not\Rightarrow b}
\end{align*}

Under this assumption a large amount of labeled training data can be generated. Because we have plenty of labeled data, we limit the scope of the problem to only consider supervised learning approaches. This delimits the problem to binary classification problem in a supervised machine learning context.

\todo{Nattiya mentioned that this section could be expanded}

\subsection{Defining the Learning Task}
In order to give a precise definition of the problem we will define it formally, before explaining what work is needed to solve it.

We first formulate the classification task as an optimization problem where we maximize the objective function $\Pr(y \mid f(a,b))$, where $a,b \in V$: 
% of $(a,b) \in \ \Rightarrow$, through this objective function:
\todo{Hun siger vi skal have reference til objective function, men Simon ved ikke lige hvilken der er god}

\begin{equation}
\label{objective}
\argmax_y\ \Pr(y \mid f(a,b)) 
\end{equation}

Where $y$ is defined as:

\[
    y=
\begin{cases}
    1 & \text{if } (a,b) \in \  \Rightarrow\\
    0 & \text{otherwise}
\end{cases}
\]

Here, 1 represents the class of article pairs that should be linked, and 0 the ones that should not.

Let $f: V\times V \to \mathbb{R}^d$ be a function mapping from article pairs to a feature representation as vector, where $d$ is the dimension of the feature vector.

We wish to find which value for $y$ that gives the highest probability for $\Pr$ given a feature representation $f(a,b)$.

%To learn predicting the label $y \in \Set{1,0}$ of a pair of articles $(a,b) \in V \times V$, we seek to find the value of $y$ that maximizes the function $\Pr(y \mid f(a,b))$, which represents the probability that articles should be linked, conditioned on the feature representation $f(x)$:

There are two parts to solving this problem. First we must train a classifier to identify which label for $y$ that gives the highest probability. But before we can do that, we must define the function $f$ that generates the feature vector. The choice of classifier will be examined in \cref{choosing_classifier}, while $f$ will be described in the following section. \todo{explain $f$ somewhere}

%\subsubsection{Article Features}
%\todo{this needs review, and should maybe be moved to another section}
%
%The function $f: V\times V \to \mathbb{R}^d$ maps article pairs to a feature representation in $d$ dimensions, by combining the features of each article pair.
%
%$f(a,b) = g(h(a), h(b))$ where $h: V \to \mathbb{R}^d$ is a function mapping an article to a feature vector and $g: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^d$ is a binary operation combining a pair of feature vectors.
%
%The feature vector $h(x)$ represents the learned features of article $x$.

%\paragraph{Input:}
%A set of $m$ training examples $(x^j,y^j)$ for $j=1,2..m$, sampled from a distribution $f(E_j)$ of article pair features, with $x^j \in R^n$ being a feature vector and $y^j \in \set{-1,+1}$ a training label. A training label $y^j=+1$ will denote a \textit{positive sample} and $y^j=-1$ a \textit{negative sample}. The i-th feature of a sample $x^j$ defined $x^j_i$ represents a pair of articles and is a combination of article features generated by node2vec~\cite{node2vec} from the training pairs $P$ and $N$.

%\todo{explain/generalize article combination without mentioning node2vec? (binary operation)}
%\todo{beskriv hvilken objective function vi vil minimere/maksimere}
%\todo{input: $x^j$ skal v√¶re par af artikler}

%node2vec:
%Given two nodes u and v, and a binary operator o' the feature vectors f(u) and f(v) are combined into the representation g(u,v).

%\paragraph{Output:}
%A function $f: R^n \to \set{-1,+1}$ that classifies additional samples ${x^k}$ sampled from $F$.