\section{Machine Learning Task}\label{sec:machine_learning_task}
%With a graph defined,
To solve the problem of suggesting links between Wikipedia articles using machine learning, we formulate it as a binary classification problem of determining whether two Wikipedia articles should be linked.

As described in \cref{sec:choice_of_graph} we assume featured articles to have appropriate linking. This makes links from featured articles suitable as positive training pairs when learning if a link should be present. Similarly articles that are not linked from a featured article can be used as negative training pairs.

In \cref{eq:training_pairs}, we define a set of positive training pairs $P \subset V \times V$, as well as a set of negative training pairs $N \subset V \times V$.

\begin{equation}
\label{eq:training_pairs}
  \begin{split}
    P &= \Set{(a,b)\ \in\ \Rightarrow\ \mid a\text{ is featured}\ } \\
    N &= \Set{(a,b)\ \in\ \Rightarrow\ \mid a\text{ is featured}\ }
  \end{split}
\end{equation}

Having these sets available as an exemplification of the Wikipedia guidelines, leads us to delimit the project scope to supervised learning, where we wish to perform binary classification of potential links.

%Under this assumption labeled training data can be generated, and so we limit the scope of the project to only consider supervised learning approaches. This delimits the problem to binary classification problem in a supervised machine learning context.

\subsection{Binary Classification}\label{sec:ml_def}
To give a precise understanding of the binary classification problem we define it formally before explaining how we intend to solve it.

Formally we seek to approximate a function $h: V \times V \to L$ where $L=\{0,1\}$ is the set of possible labels. $1$ represents the class of article pairs that should be linked, and $0$ the ones that should not.

As classifiers operate on feature vectors, we first need to represent article pairs as appropriate feature vectors. To do this we define a function $f: V\times V \to \mathbb{R}^d$, mapping article pairs to $d$-dimensional feature vectors.

With the function $f$ we seek to learn a function $g: \mathbb{R}^d \to L$, that assigns a label $y_{ab} \in L$ to a feature representation $x_{ab} \in \mathbb{R}^d$, for a given article pair $(a,b) \in V \times V$, such that \cref{eq:labels} holds.

\begin{equation}
\label{eq:labels}
y_{ab}=\mathbb{1}_{a \Rightarrow b}
\end{equation}

%\begin{equation}
%\label{eq:labels}
%    y_{ab}=
%    \begin{cases}
%        1 & \text{if } a \Rightarrow b\\
%        0 & \text{otherwise}
%    \end{cases}
%\end{equation}

Using the functions $f$ and $g$, we can define the function $h$ as shown in \cref{eq:h_func}.

\begin{equation}
\label{eq:h_func}
  h = g \circ f
\end{equation}

 We can then use the function $h$ to assign a label $y_{ab}$ to any article pair $(a,b)$ as shown in \cref{eq:classification_function}.

\begin{equation}
\label{eq:classification_function}
  y_{ab} = h(a,b)
\end{equation}

There are two parts to solving this problem. We must train a classifier to approximate the function $g$, identifying which label $y_{ab}$ should be given to a feature representation $x_{ab}$. Before we can do that, we must define the function $f$ that generates the feature representation. The choice of classifier will be examined in \cref{choosing_classifier}, while $f$ will be described in the following section.

%We wish to find which value for $y$ that gives the highest probability for $\Pr$ given a feature representation $f(a,b)$.

%To learn predicting the label $y \in \Set{1,0}$ of a pair of articles $(a,b) \in V \times V$, we seek to find the value of $y$ that maximizes the function $\Pr(y \mid f(a,b))$, which represents the probability that articles should be linked, conditioned on the feature representation $f(x)$:

%There are two parts to solving this problem. We must train a classifier to identify which label for $y$ that gives the highest probability. But before we can do that, we must define the function $f$ that generates the feature vector. The choice of classifier will be examined in \cref{choosing_classifier}, while $f$ will be described in the following section.

%\subsubsection{Article Features}
%\todo{this needs review, and should maybe be moved to another section}
%
%The function $f: V\times V \to \mathbb{R}^d$ maps article pairs to a feature representation in $d$ dimensions, by combining the features of each article pair.
%
%$f(a,b) = g(h(a), h(b))$ where $h: V \to \mathbb{R}^d$ is a function mapping an article to a feature vector and $g: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^d$ is a binary operation combining a pair of feature vectors.
%
%The feature vector $h(x)$ represents the learned features of article $x$.

%\paragraph{Input:}
%A set of $m$ training examples $(x^j,y^j)$ for $j=1,2..m$, sampled from a distribution $f(E_j)$ of article pair features, with $x^j \in R^n$ being a feature vector and $y^j \in \set{-1,+1}$ a training label. A training label $y^j=+1$ will denote a \textit{positive sample} and $y^j=-1$ a \textit{negative sample}. The i-th feature of a sample $x^j$ defined $x^j_i$ represents a pair of articles and is a combination of article features generated by node2vec~\cite{node2vec} from the training pairs $P$ and $N$.

%\todo{explain/generalize article combination without mentioning node2vec? (binary operation)}
%\todo{beskriv hvilken objective function vi vil minimere/maksimere}
%\todo{input: $x^j$ skal v√¶re par af artikler}

%node2vec:
%Given two nodes u and v, and a binary operator o' the feature vectors f(u) and f(v) are combined into the representation g(u,v).

%\paragraph{Output:}
%A function $f: R^n \to \set{-1,+1}$ that classifies additional samples ${x^k}$ sampled from $F$.
