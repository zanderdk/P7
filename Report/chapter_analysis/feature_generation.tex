\section{Feature Generation}\label{sec:feature_generation}
Generating an appropriate feature representation is an integral part of applying machine learning techniques, and it is essential to the performance of the machine learning algorithms~\cite{ng-lecture}. The feature representation aims to discriminate the data so that a desired structure appears. This section describes our approach to generating this feature representation for articles pairs, described by the function $f$ in \cref{sec:ml_def}.

\subsection{Feature Engineering}
Feature engineering is the process of identifying information in a dataset that has a high discriminative power. In a binary classification problem, such information would be a set of features capable of separating positive and negative data points. Since a good choice of features may not be obvious, identifying these features often requires domain knowledge, and intuition of the problem domain~\cite{ng-lecture,domingos2012few}. Feature engineering can often be a time consuming process, and many trial and error iterations may be needed to derive appropriate features. Below we have listed some ideas for features we have explored, but there are many more than these.

\begin{description}
    \item[Term similarity] This is a metric that describes the similarity of the most important terms on two given Wikipedia articles. A term is a word in an article, that is deemed significant to that article. By identifying the terms of each article, a term similarity score between two articles can be computed. The idea behind the feature is that two articles that use very similar terms, might cover similar topics, which could warrant a link.
    \item[Shortest path] When considering Wikipedia as a graph, the shortest path distance between two articles could be used as a feature. The idea is that there is a correlation between the shortest path distance of two articles \emph{A} and \emph{B}, and whether there is a link between \emph{A} and \emph{B}. This feature might be able to indicate whether two articles are related by subject.
    \item[Common direct predecessors/successors] The motivation of these two features is that two articles may be related to such a degree that they should be linked, if they have many common direct predecessors or successors. A graphical representation of these features is shown in \cref{fig:parent-rel,fig:children-rel}.
\end{description}

\begin{figure}[tbp]%
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{parents}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]10:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]150:}
              ] (p) {P};
      \node [node, below left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]100:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]200:}
              ] (a) {A};
      \node [node, below right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]40:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]85:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]200:}
              ] (b) {B};
      \path [line, very thick] (p) -- (a);
      \path [line, very thick] (p) -- (b);
    \end{tikzpicture}
    \caption[Common direct predecessor]{P is a common direct predecessor of A and B}%
    \label{fig:parent-rel}%
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{children}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]350:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]210:}
              ] (p) {S};
      \node [node, above left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]15:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]230:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]280:}
              ] (a) {A};
      \node [node, above right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]280:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:}
              ] (b) {B};
      \path [line, very thick] (a) -- (p);
      \path [line, very thick] (b) -- (p);
    \end{tikzpicture}
    \caption[Common direct successor]{S is a common direct successor of A and B}%
    \label{fig:children-rel}%
  \end{minipage}


\end{figure}

The process of feature engineering did however prove to be rather time consuming, and the usefulness of the resulting features was difficult to reason about, as we do not possess the required domain knowledge. As such we are unsure whether we can develop features of sufficient quality within the allotted project time. Due to this we examined alternative approaches to generate this feature representation, namely feature learning.

\subsection{Feature Learning}
Feature learning is an approach in machine learning that has shown impressive results in many applications~\cite{bengio2013representation}. It is a method for generating feature representations in an automated fashion from raw input data, avoiding the need for the time consuming feature engineering process~\cite{ng-lecture}. Additional advantages of using feature learning are a reduced need for domain knowledge, and the ability to uncover structural patterns that are hard to discover manually. This is an attractive idea; obtaining good features can be automated.

The features generated by feature learning aims to be suited for computational reasoning by machine intelligence algorithms, but can be difficult to interpret by humans. In practice this has been shown to be of minor concern, as many applications have experienced greatly improved performance using feature learning~\cite{bengio2013representation}.

This high level of automation creates the vision of having declarative machine learning, where the only required input is the dataset and a metric for classification, from which both the process of identifying features and classifying the data can be completed autonomously given enough computational power. The possibility of such a system might be the most significant benefit of feature learning. Each step of automation in the machine learning pipeline however increases the difficulty of understanding why a certain approach in the pipeline does not yield the expected outcome.
\todo{Synes stadig der her er lidt overdrevet, men den sidste pointe er god}

Compared to feature engineering, feature learning offers several benefits for this project. The reduced time requirements, along with the reduced need for domain knowledge, could allow for generation of more and better discriminating features. Due to these benefits we seek to learn the function $f$ that generates the feature representation using feature learning, instead of defining it using feature engineering.
