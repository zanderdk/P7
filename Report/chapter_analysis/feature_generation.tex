\section{Feature Generation}\label{sec:feature_generation}
Generating an appropiate feature representation is an integral part of applying machine learning techniques~\cite{ng-lecture}. The feature representation aims to discriminate the data so that a wanted structure appears. Deriving a suitable feature representation of the data set is paramount to the performance of the machine learning algorithm~\cite{ng-lecture}. \todo{kan man bruge paramount s√•dan?} This section describes our aprroach to generating this feature representation.

%Traditionally, features are engineered from a large data set\todo{source}. This means that features deemed characteristic for a given problem are identified manually from the data set. In recent years, feature learning has gained popularity\todo{source}. Here features are automatically generated from the structure of a data set.

%In the following sections we will discuss strengths and weaknesses of each approach to feature generation, before coming to a conclusion on how they are used in our project.

\subsection{Feature Engineering}
The premise of feature engineering is that domain experts are able to identify information in a data set that has a high degree of discrimination power. For example, in a binary classification problem, such information would be a set of features good at separating positive and negative examples. Identifying these features require domain knowledge, since a good choice of features may not be obvious~\cite{ng-lecture}. This identification can often be a time consuming process, and many trial and error iterations may be needed to derive appropiate features.

We do not have expert domain knowledge of Wikipedia, but below we have listed some ideas for features that according to our intuition could be worth exploring.

\begin{description}
    %\item[Clickstream] \todo{describe what this is before? link to wikipedia clickstream and maybe alexanders regression paper}.
    \item[Term similarity] This is a metric that describes the similarity of the most important terms on two given Wikipedia articles. A term is a word in an article, that is deemed significant to that article. If article \emph{A} has the terms \emph{w1}, \emph{w2}, \emph{w3} and article \emph{B} has \emph{w3}, \emph{w4}, a metric could be the number of similar terms, which in this case is $1$. The idea behind the feature, is that there is a correlation between the term similarity of articles and whether they should be linked.
    \item[Shortest path] Consider Wikipedia as a graph, where the vertices are articles and edges are links between articles. The idea of this feature is that there is a correlation between the shortest path of two articles \emph{A}, \emph{B}, and whether there is a link between \emph{A} and \emph{B}. We expect that this feature will be most useful, in cases where the shortest is big, since this would likely mean that the articles are not related by subject, since they are not in the same graph neighbourhood.
    \item[Common direct predecessors/succesors] The motivation of this feature is that if two articles have common direct predecessors or successors, they may be related to such a degree that they should be linked. A graphical representation of the features is shown in \cref{fig:parent-rel} and \cref{fig:children-rel}.
\end{description}

\begin{figure}[tbp]%
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{parents}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]10:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]150:}
              ] (p) {P};
      \node [node, below left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]100:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]200:}
              ] (a) {A};
      \node [node, below right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]40:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]85:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]200:}
              ] (b) {B};
      \path [line, very thick] (p) -- (a);
      \path [line, very thick] (p) -- (b);
    \end{tikzpicture}
    \caption[short desc]{A figure showing a common direct predecessor of A and B}%
    \label{fig:parent-rel}%
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{children}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]350:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]210:}
              ] (p) {S};
      \node [node, above left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]15:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]230:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]280:}
              ] (a) {A};
      \node [node, above right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]280:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:}
              ] (b) {B};
      \path [line, very thick] (a) -- (p);
      \path [line, very thick] (b) -- (p);
    \end{tikzpicture}
    \caption[short desc]{A figure showing a common direct successor of A and B}%
    \label{fig:children-rel}%
  \end{minipage}


\end{figure}

The process of manual feature engineering did however prove to be rather time consuming, and the suitability of the resulting features was difficult to reason about, due to our limited domain knowledge. Due to this we examined alternative approaches to generate this feature representation, namely feature learning.

\subsection{Feature Learning}
Feature learning is a method for generating the feature representation automatically, avoiding the need for manual feature engineering~\cite{ng-lecture}. Instead of manually deriving discriminating features, they are generated by the structure in the data set. This has the advantage that the need for domain knowledge is reduced, and the feature learning algorithm is able to uncover structural patterns that are hard for humans to discover. In return, as the features are generated, they can be difficult to reason about. In other words, there is limited intuitive understanding available that can explain what each individual feature means. As the feature learning process is mostly autonomous, it is possible to systematically try many combinations of parameters for the algorithm, in order to improve the feature representation.

As described in \cref{sec:choice_of_graph} it was chosen to structure Wikipedia articles and links between them as a graph. Due to this structure in our information we chose to examine feature learning techniques working on graph structures, also called network embedding.

While there exists many more feature learning techniques, but we found network embedding the most relevant, and we use this method to generate the feature representation.
