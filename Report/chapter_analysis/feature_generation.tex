\section{Feature Generation}\label{sec:feature_generation}
A main component of any machine learning technique is features. The features should be used to discriminate the data so that a wanted structure appears. Deriving good features from a data set is paramount to the performance of the machine learning algorithm.

Traditionally, features are engineered from a large data set. This means that particular parts of the data set that are deemed characteristic for a given problem are hand-picked by humans. In recent years, feature learning has gained popularity. In short, features are automatically generated by looking at the structure of a data set.

In the following sections we will discuss strengths and weaknesses of each approach to feature generation, before common to a conclusion on how they are used in our project.

\subsection{Feature Engineering}
The premise of feature engineering is that humans are able to identify information in a data set that has a high degree of discrimination power. For example, in a binary classification problem, \enquote{information with a high degree of discrimination power} would be a set of features, that are good at separating positive and negative examples. Identifying these features usually require domain knowledge, since a good choice of features may not be obvious. This identification can often be a time consuming process, and many trial and error iterations can be needed in order to derive good features.

We do not have expert domain knowledge of Wikipedia, but below we have listed some ideas for features that according to our intuition could worth exploring.

\begin{description}
    \item[Clickstream] \todo{describe what this is before? link to wikipedia clickstream and maybe alexanders regression paper}.
    \item[Terms similarity] This is a metric that describes the similarity of the most important terms on two given Wikipedia articles. For example, if article A has the terms w1, w2, w3 and article B has w3, w4, a metric could be the number of similar terms, in this case 1. The idea behind the feature is that articles with similar terms are more likely to be linked together.
    \item[Shortest path] Consider Wikipedia as a graph, where the vertices are articles and edges are links between articles. The idea of this feature is that there is a correlation between the shortest path of two articles A, B, and whether there is a link between A and B. 
    \item[Common direct predecessors/succesors] See \cref{fig:children-rel} and \cref{fig:parent-rel}. The motivation of this feature is that if two articles have common direct predecessors or successors they may be related to such a degree that they should be linked. \todo{Explain the figure.}
\end{description}


\begin{figure}[tbp]%
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{parents}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]10:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]150:}
              ] (p) {P};           
      \node [node, below left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:}, 
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]100:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]200:}
              ] (a) {A};           
      \node [node, below right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:}, 
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]40:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]85:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]200:}
              ] (b) {B};
      \path [line, very thick] (p) -- (a);
      \path [line, very thick] (p) -- (b);
    \end{tikzpicture}
    \caption[short desc]{A figure showing a common direct predecessor of A and B}%
    \label{fig:parent-rel}%
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{children}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]350:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]210:}
              ] (p) {S};           
      \node [node, above left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]15:}, 
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]230:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]280:}
              ] (a) {A};           
      \node [node, above right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:}, 
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]280:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:}
              ] (b) {B};
      \path [line, very thick] (a) -- (p);
      \path [line, very thick] (b) -- (p);
    \end{tikzpicture}
    \caption[short desc]{A figure showing a common direct successor of A and B}%
    \label{fig:children-rel}%
  \end{minipage}
  

\end{figure}

\subsection{Feature Learning}
The obvious advantage of feature learning is that it obviates the need for feature engineering. Instead of manually deriving discriminating information, features are generated by the structure of the data set. This has the advantage that the need for domain knowledge is reduced, and the feature learning algorithm is able to uncover structural patterns that are hard for humans to discover. In return, as the features are generated, they can be difficult to reason about. In other words, there is limited intuitive understanding available that can explain what each individual feature means. As the feature learning process is mostly autonomous, it is possible to systematically try many combinations of parameters for the algorithm, in order to find the best set of features.

As described in \todo{some section} it was chosen to structure Wikipedia articles and links between them as a graph. Due to this structure in our information we chose to examine feature learning techniques working on graph structures also called network embedding.
We are aware that many more feature learning techniques exists, but we found this type of feature learning the most relevant and chose to delimit to only focusing on network embedding.

\section{Our choice/idea/solution/guess}
Wikipedia contains a big amount of data that represents different types of information. Articles have many different attributes and embeddings of information as well as editor and revision history. Attributes such as categories and article links provides structural information that can be used to describe the connection and relation of things.

Through feature engineering we believe that we can identify specific pieces of information that can be used as machine learning features. Because of our limited domain knowledge these features might not be adequate, but we believe that by utilizing feature learning on this information, useful features can be learned.

It is our intuition that the link-structure of Wikipedia can provide some of the most promising features for predicting missing links. Furthermore we believe that the Wikipedia clickstream will be beneficial because it captures the user behaviour of how links are used.

\paragraph{Plan:} We will be building a clickstream graph of Wikipedia that represents how articles are linked and navigated by users. Using this graph we will use feature learning to learn features from the structure, which will be used to train a classifier for suggesting missing article links.


\todo{other features, methods (text content...)}

\todo{insert clickstream source / motivation}


