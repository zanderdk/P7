\section{Feature Generation}\label{sec:feature_generation}
Generating an appropriate feature representation is an integral part of applying machine learning techniques, and it is essential to the performance of machine learning algorithms~\cite{ng-lecture}. The feature representation aims to allow the classifier to discriminate the data so that a desired structure emerges. This section describes approaches to generate this feature representation for articles pairs, described by the function $f$.

\subsection{Feature Engineering}
Feature engineering is the process of identifying information in a dataset that has a high discriminative power, and finding a way to represent this information as features \todo{Find kilde!}. In a binary classification problem, such information would be a set of features capable of separating two classes of data points. Since a good choice of features may not be obvious, identifying these features often requires domain knowledge and intuition of the problem domain~\cite{ng-lecture,domingos2012few}. Feature engineering can often be a time consuming process, and many iterations of trial and error may be needed to derive appropriate features. Below we briefly describe some feature ideas we have explored.

\begin{description}
    \item[Term similarity] This feature describes the similarity of significant terms on two given Wikipedia articles. By identifying the terms of each article, a term similarity score between the two articles can be computed. The idea behind the feature is that two articles that use very similar terms might cover similar topics, which could warrant a link.
    \item[Shortest distance] When considering Wikipedia as a graph, the distance of the shortest path between two articles could be used as a feature. The idea is that there might be a correlation between the shortest distance of two articles \emph{a} and \emph{b}, and whether they should be linked. \todo{vi kan ikke lide at den er vag og tilf√∏j negativ eksaemlasdfnjk} This feature might be able to indicate whether two articles are related by subject.
    \item[Common direct predecessors/successors] The motivation for these two features is that whether two articles should be linked, could be influenced by the number of common direct predecessors or successors. Illustrations of these features are shown in \cref{fig:parent-rel,fig:children-rel}.
\end{description}

\begin{figure}[tbp]%
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{parents}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]10:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]150:}
              ] (p) {$p$};
      \node [node, below left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]100:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]200:}
              ] (a) {$a$};
      \node [node, below right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]40:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]85:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]200:}
              ] (b) {$b$};
      \path [line, very thick] (p) -- (a);
      \path [line, very thick] (p) -- (b);
    \end{tikzpicture}
    \caption[Common direct predecessor]{$p$ is a common direct predecessor of $a$ and $b$}%
    \label{fig:parent-rel}%
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{children}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]350:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]210:}
              ] (p) {$s$};
      \node [node, above left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]15:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]230:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]280:}
              ] (a) {$a$};
      \node [node, above right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]280:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:}
              ] (b) {$b$};
      \path [line, very thick] (a) -- (p);
      \path [line, very thick] (b) -- (p);
    \end{tikzpicture}
    \caption[Common direct successor]{$s$ is a common direct successor of $a$ and $b$}%
    \label{fig:children-rel}%
  \end{minipage}


\end{figure}

The process of feature engineering did however prove to be rather time consuming, and the usefulness of the resulting features was difficult to reason about, as we do not possess the required domain knowledge. As such, we are unsure whether we can develop features of sufficient quality. Due to this, we examined an alternative approach, feature learning, to generate this feature representation.

\subsection{Feature Learning}
Feature learning is an approach in machine learning that has shown promising results in many applications~\cite{bengio2013representation}. It is an approach for automatically generating feature representations from input data, avoiding the need for the time consuming feature engineering process~\cite{ng-lecture}. Additional advantages of using feature learning are a reduced need for domain knowledge, and the ability to uncover structural patterns that are hard to discover manually.

However, an increased level of automation makes it more difficult to understand the result of the machine learning process. In practice this has been shown to be of minor concern, as many applications have experienced greatly improved performance using feature learning~\cite{bengio2013representation}.

%This high level of automation creates the vision of having declarative machine learning, where the only required input is the dataset and a metric for classification, from which both the process of identifying features and classifying the data can be completed autonomously given enough computational power. The possibility of such a system might be the most significant benefit of feature learning.

Compared to feature engineering, feature learning offers several benefits for this project. The reduced time requirements, along with the reduced need for domain knowledge, could allow generation of more and better discriminating features. Due to these benefits, we seek to learn the function $f$ that generates the feature representation using feature learning, instead of defining it using feature engineering.
