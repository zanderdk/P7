\section{Feature Generation}\label{sec:feature_generation}
This section describes how the function $f: V\times V \to \mathbb{R}^d$ from the last section can be definde, utilising manual engineering or through feature learning.
Generating an appropriate feature representation is an integral part of applying machine learning techniques~\cite{ng-lecture}. The feature representation aims to discriminate the data so that a wanted structure appears. Deriving a suitable feature representation of the data set is essential to the performance of the machine learning algorithm~\cite{ng-lecture}. This section describes our approach to generating this feature representation.

%Traditionally, features are engineered from a large data set\todo{source}. This means that features deemed characteristic for a given problem are identified manually from the data set. In recent years, feature learning has gained popularity\todo{source}. Here features are automatically generated from the structure of a data set.

%In the following sections we will discuss strengths and weaknesses of each approach to feature generation, before coming to a conclusion on how they are used in our project.

\subsection{Feature Engineering}
The premise of feature engineering is that it is possible to identify information in a dataset that has a high degree of discrimination power. For example, in a binary classification problem, such information would be a set of features capable of separating positive and negative examples. Identifying these features requires expert domain knowledge, since a good choice of features may not be obvious~\cite{ng-lecture}. This identification can often be a time consuming process, and many trial and error iterations may be needed to derive appropriate features.

We do not have expert domain knowledge of Wikipedia, but below we have listed some ideas for features that according to our intuition could be worth exploring.

\begin{description}
    %\item[Clickstream] \todo{describe what this is before? link to wikipedia clickstream and maybe alexanders regression paper}.
    \item[Term similarity] This is a metric that describes the similarity of the most important terms on two given Wikipedia articles. A term is a word in an article, that is deemed significant to that article. If article \emph{A} has the terms $w_1$, $w_2$, $w_3$ and article \emph{B} has $w_3$, $w_4$, a metric could be the number of similar terms, which in this case is $1$. The idea behind the feature, is that there is a correlation between the term similarity of articles and whether they should be linked.
    \item[Shortest path] Consider Wikipedia as a graph, where the vertices are articles and edges are links between articles. The idea of this feature is that there is a correlation between the shortest path of two articles \emph{A}, \emph{B}, and whether there is a link between \emph{A} and \emph{B}. We expect this feature to be most useful when the shortest path is long, which could indicate that the articles are not related by subject, since they are not in the same graph neighborhood.
    \item[Common direct predecessors/successors] The motivation of this feature is that if two articles have common direct predecessors or successors, they may be related to such a degree that they should be linked. A graphical representation of the features is shown in \cref{fig:parent-rel,fig:children-rel}.
\end{description}

\begin{figure}[tbp]%
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{parents}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]10:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]150:}
              ] (p) {P};
      \node [node, below left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]100:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]200:}
              ] (a) {A};
      \node [node, below right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]40:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]85:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]200:}
              ] (b) {B};
      \path [line, very thick] (p) -- (a);
      \path [line, very thick] (p) -- (b);
    \end{tikzpicture}
    \caption[Common direct predecessor]{P is a common direct predecessor of A and B}%
    \label{fig:parent-rel}%
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{children}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]350:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]210:}
              ] (p) {S};
      \node [node, above left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]15:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]230:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]280:}
              ] (a) {A};
      \node [node, above right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]280:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:}
              ] (b) {B};
      \path [line, very thick] (a) -- (p);
      \path [line, very thick] (b) -- (p);
    \end{tikzpicture}
    \caption[Common direct successor]{S is a common direct successor of A and B}%
    \label{fig:children-rel}%
  \end{minipage}


\end{figure}

The process of manual feature engineering did however prove to be rather time consuming, and the suitability of the resulting features was difficult to reason about, due to our limited domain knowledge. Due to this we examined alternative approaches to generate this feature representation, namely feature learning.

\subsection{Feature Learning}
Representation learning, or feature learning as we will term it, is a recent approach in machine learning that has showcased impressive results in many applications~\cite{bengio2013representation}. It a method for generating feature representations from raw input data, avoiding the need for manual feature engineering~\cite{ng-lecture}. Instead of manually deriving discriminating features, they are generated by the structure of the dataset. This has the advantage that the need for domain knowledge is reduced, and the feature learning algorithm is able to uncover structural patterns that are hard for humans to discover. In addition, feature engineering is a labor intensive part of constructing machine intelligence pipelines, that can be reduced by feature learning. This allows for greater exploration of other parts of the machine learning pipeline.

The features generated by feature learning aims to be suited for computational reasoning by machine intelligence algorithms, but can be difficult to interpret by humans. In other words, there is limited intuitive understanding available that can explain what each individual feature means. This has however in practice shown not to be a big problem, as many applications have experienced greatly improved performance using feature learning~\cite{bengio2013representation}. Because the feature learning process is mostly autonomous, the parameter space of the feature learning algorithm can be systematically explored, in order to improve the feature representation without understanding the features generated.

As described in \cref{sec:choice_of_graph} we choose to structure Wikipedia articles and links between them as a graph. Due to this structure, we naturally explore feature learning on graphs also called network embedding. Algorithms in this category generate features based on a graph.  While there exists many more feature learning techniques, we found network embedding the most relevant to our problem, and we use this method to generate the feature representation.
So in our case we seek to learn the function $f: V\times V \to \mathbb{R}^d$ based on the graph representation of Wikipedia $G$.
We will further elaborate on network embedding and describe a concrete feature learning algorithm in \cref{feature_extractor}.


