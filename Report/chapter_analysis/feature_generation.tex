\section{Feature Generation}\label{sec:feature_generation}
Generating an appropriate feature representation is an integral part of applying machine learning techniques, and it is essential to the performance of the machine learning algorithms~\cite{ng-lecture}. The feature representation aims to discriminate the data so that a desired structure appears. This section describes our approach to generating this feature representation for articles pairs, described by the function $f$ in \cref{sec:ml_def}.

\subsection{Feature Engineering}
Feature engineering is the process of identifying information in a dataset that has a high discriminative power. In a binary classification problem, such information would be a set of features capable of separating positive and negative data points. Identifying these features often requires domain knowledge, since a good choice of features may not be obvious~\cite{ng-lecture}. Feature engineering can often be a time consuming process, and many trial and error iterations may be needed to derive appropriate features. Below we have listed some ideas for features we have explored.

\begin{description}
    \item[Term similarity] This is a metric that describes the similarity of the most important terms on two given Wikipedia articles. A term is a word in an article, that is deemed significant to that article. By identifying the terms of each article, a term similarity score between two articles can be computed. The idea behind the feature is that two articles that use very similar terms, might cover similar topics, which could warrant a link.
    \item[Shortest path] When considering Wikipedia as a graph, the shortest path distance between two articles could be used as a feature. The idea is that there is a correlation between the shortest path distance of two articles \emph{A} and \emph{B}, and whether there is a link between \emph{A} and \emph{B}. This feature might be able to indicate whether two articles are related by subject.
    \item[Common direct predecessors/successors] The motivation of these two features is that two articles may be related to such a degree that they should be linked, if they have many common direct predecessors or successors. A graphical representation of these features is shown in \cref{fig:parent-rel,fig:children-rel}.
\end{description}

\begin{figure}[tbp]%
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{parents}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]10:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]150:}
              ] (p) {P};
      \node [node, below left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]100:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]200:}
              ] (a) {A};
      \node [node, below right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]40:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]85:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]200:}
              ] (b) {B};
      \path [line, very thick] (p) -- (a);
      \path [line, very thick] (p) -- (b);
    \end{tikzpicture}
    \caption[Common direct predecessor]{P is a common direct predecessor of A and B}%
    \label{fig:parent-rel}%
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{children}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]350:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]210:}
              ] (p) {S};
      \node [node, above left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]15:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]230:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]280:}
              ] (a) {A};
      \node [node, above right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]280:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:}
              ] (b) {B};
      \path [line, very thick] (a) -- (p);
      \path [line, very thick] (b) -- (p);
    \end{tikzpicture}
    \caption[Common direct successor]{S is a common direct successor of A and B}%
    \label{fig:children-rel}%
  \end{minipage}


\end{figure}

The process of feature engineering did however prove to be rather time consuming, and the suitability of the resulting features was difficult to reason about, due to our limited domain knowledge. Due to this we examined alternative approaches to generate this feature representation, namely feature learning.
\todo{Rewrite this paragraph to avoid suitability and limited}

\todo{Det er et tyndt afsnit, tyndt grundlag. Udvid med forklaring af anerkendte teknikker/metoder.}

\subsection{Feature Learning}
Feature learning is an approach in machine learning that has shown impressive results in many applications~\cite{bengio2013representation}. It is a method for generating feature representations from raw input data, avoiding the need for the time consuming feature engineering process~\cite{ng-lecture}. Additional advantages of using feature learning are a reduced need for domain knowledge, and the ability to uncover structural patterns that are hard to discover manually.

The features generated by feature learning aims to be suited for computational reasoning by machine intelligence algorithms, but can be difficult to interpret by humans. In practice this has been shown to be of minor concern, as many applications have experienced greatly improved performance using feature learning~\cite{bengio2013representation}.

Algorithms for feature learning typically have hyperparameters that can be used to tune the way features are generated. The process of finding hyperparameters that yields improved feature representations can be systematically explored, which is an attractive idea; Obtaining good features can be automated.

This high level of automation creates the vision of having declarative machine learning, where the only required input is the dataset and a metric for classification, from which both the process of identifying features and classifying the data can be completed autonomously given enough computational power. The possibility of such a system might be the most significant benefit of feature learning. Each step of automation in the machine learning pipeline however increases the difficulty of understanding why a certain approach in the pipeline does not yield the expected outcome.
\todo{meh}

Compared to feature engineering, feature learning offers several benefits for this project. The reduced time requirements, along with the reduced need for domain knowledge, could allow for generation of more and better discriminating features. Due to these benefits we seek to learn the function $f$ that generates the feature representation using feature learning, instead of defining it using feature engineering.

As we need to learn feature representations for any arbitrary article pair $(a,b) \in V \times V$, both where $a \Rightarrow b$ and $a \not \Rightarrow b$, using feature learning requires an approach that supports this. A popular approach for feature learning on graph structures that supports this is \emph{network embedding}.\todo{source}

Network embedding aims to construct a low dimensional vector representation of each node in a given network, that preserves the original network structure~\cite{sun2016general}. This vector can then be used as a feature representation of the corresponding node. The feature representation $v_{ab}$ for any node pair $(a,b) \in V \times V$ can then be found by combining the feature vectors for the two nodes with an arbitrary binary operation $v_{a} \circ v_{b}$, where $v_{a}$ and ${v_b}$ are the feature vectors for node $a$ and $b$, respectively.

% With the
% Network embedding suits the problem of learning features


% Therefore, we use network embedding to learn the function $f$. As there are different methods for network embedding available, the specific ch

% Due to the advantages of using feature learning, we seek to learn the function $f$ by using network embedding on the graph representation of Wikipedia, instead of defining it manually using feature engineering. The specific network embedding we use will be described in \cref{feature_extractor}.
