%This paragraph covers the first point of the intro from the writing guide
The purpose of the feature extractor component is to generate a feature representation of a candidate article pair, as described by the function $f$ in \cref{sec:ml_def}, that can be used for classification. This is done through a feature learning approach, as chosen in \cref{sec:feature_generation}.

% As we need to learn feature representations for any arbitrary article pair $(a,b) \in V \times V$, both where $a \Rightarrow b$ and $a \not \Rightarrow b$, using feature learning requires an approach that supports this. A popular approach for feature learning on graph structures that supports this is \emph{network embedding}.\todo{source} Network embedding aims to construct a low dimensional vector representation of each node in a given network, that preserves the original network structure~\cite{sun2016general}. This vector can be used as a feature representation of the node. The feature representation $v_{a,b}$ for any node pair $(a,b) \in V \times V$, can then be found by combining the individual feature vectors using an arbitrary binary operation $v_{a} \circ v_{b}$, where $v_{a}$ and ${v_b}$ are the feature vectors for node $a$ and $b$, respectively.


%The two paragraphs below covers the second point of the writing guide
To generate the features, we use an algorithmic framework for semi-supervised feature learning called node2vec~\cite{node2vec}. It is used to learn feature vectors for nodes based on graph structure, which can then be used to identify characteristics of a relation between two nodes. After identifying a feature representation of these characteristics, the classifier component can use them as input features. We choose to use node2vec based on reports of its performance as well as its flexibility to adapt to different graph structures.

In this section we cover a basic overview of node2vec, in order to consider the implications of using node2vec for our purposes. Afterwards, we cover how we adapt node2vec to our specific problem, and finally we consider the implementation of the feature learner support module and the feature extractor.


%The next component in the main pipeline is the feature extractor. This component generates the feature representation of a candidate pair as described by the function $f$ in \cref{sec:ml_def}, which can then be used for classification.

%The feature extractor returns a single feature vector, representing the link between a given source and target article pair. As we need to classify both existing and non-existing links, it must be possible to extract feature vectors in both cases. Therefore the feature vector for a link is constructed by combining the feature vectors of the source and target articles.


%As mentioned in \cref{sec:feature_generation} we aim to use feature learning, specifically network embedding, to learn these feature representations. While there are several different ways to approach network embedding, we found node2vec~\cite{node2vec} to be suitable. The main advantage of node2vec is increased flexibility compared to other well-known approaches due to tunable hyperparameters~\cite{node2vec}, which allows us to experiment with different neighborhood exploration methods Additionally, node2vec offers highly competitive performance, and performs well for large networks~\cite{node2vec}.

%This section describes how node2vec is used to generate the feature representation, along with the process of optimizing the models hyperparameters.

%node2vec is an algorithmic framework for semi-supervised feature learning in networks~\cite{node2vec}. The goal of the algorithm is to learn feature representations for nodes in a network. To find feature representations for edges, the feature representation for two nodes can be combined. The node2vec paper proposes a way of generating features representations for nodes based on their neighborhoods. The idea is to use word embedding, where nodes are used as words, and node sequences are used as sentences. The neighborhood of a node consists of the nodes that are close in the node sequence. In the following sections we briefly describe word embedding, and how node sequences are constructed using biased random walks in the graph. Furthermore we describe the approaches to combine node vectors presented in \cite{node2vec}, and propose a new combination method tailored for this project.
