\section{Feature Extractor}\label{feature_extractor}
%Here we input the intro text.
\input{chapter_design/feature_extractor_intro}
\input{chapter_design/feature_extractor_n2v}
\input{chapter_design/feature_extractor_adjust}

\subsection{Hyperparameter Optimization}\label{sec:hyperopt}
As described in \cref{sec:node2vec}, node2vec has many tunable hyperparameters. An overview of these can be seen in \cref{tab:node2vecparams}. In addition to the hyperparameters we will also examine the performance of different binary operations for combining node feature vectors. To find the parameters that give the best feature representation, we do a hyperparameter optimization pass. We first specify the parameter space that should be searched in. As this is a large space, it is not feasible to exhaust all possibilities. We therefore use the tool called Spearmint~\cite{snoek2012practical}, which performs Bayesian optimization by maximizing the expected improvement, to efficiently search for parameters that will minimize an objective function. The objective function is $1 - \text{F-score}$, where the F-score is found by running a classifier on the node2vec model learned from the parameters. By keeping the classifier and its parameters constant, we can find the best set of parameters by finding the lowest objective function value.

\begin{table}[tbp]
\centering
\begin{tabular}{@{}lp{.75\textwidth}@{}}
\toprule
\textbf{Parameter} & \textbf{Description} \\
\midrule
$p$          &   Return parameter that controls the likelihood of immediately revisiting the previous node in the walk~\cite{node2vec}.   \\
$q$          &   Controls to which degree the walk prefers to stay in the neighborhood or to explore outwards.   \\
$d$          &   The dimensionality of the learned features.   \\
$r$          &   The number of random walks per node.   \\
$k$          &   The size of the context window.   \\
$l$          &   The max length of the random walk.   \\
function     &   The function to use for combining node features, it can be hadamard product, hadamard divide or concatenate.   \\
\bottomrule
\end{tabular}
\caption[Description of parameters in node2vec]{Description of the various parameters in node2vec}%
\label{tab:node2vecparams}
\end{table}


%\todo{Beskriv at vi optimere på et sub sæt af graphen og at det er pga speed up.}
%\todo{del mængden er featured => good og det er fordi at så behøver vi ikke lave features for all noder men kun featured og good.}
In order to run a sufficient amount of parameter experiments within a reasonable timespan, we speed up each iteration of the optimization.
This is done by limiting node2vec to only consider a subset of the graph, as well as constraining the set of nodes a walk can start from. The subset consists of the set of featured and good articles, and the set of start nodes is featured articles. By doing this, features are only generated for featured and good articles, which greatly improves performance and still makes the algorithm able to walk several nodes because these types of articles can be expected to be reasonably linked. Due to this limitation it is only possible to find features for featured and good articles, this means that we can only run parameter experiments on the subset $\{(a,b) \in P \cup N \mid \text{b is good}\}$ of our training data.

%To speed up each iteration of the optimization, we limit the set of start nodes to featured articles.
The parameter optimization is entirely autonomous and the process should be able to optimize the the parameters automatically. This means that one can learn on graph networks without knowing the underlying graph structure.

We start with a coarse search to find a good range for some parameters. This is done with large grained values for each parameter that would show local optima. After this, we search a second time with finer grained values to further refine the local optima. After completing 83 experiments the objective function seemed to have converged, as shown on \cref{fig:spearmint}. The best parameters found are shown in~\cref{tab:paramopt_goodvalues}.

\begin{figure}%
  \centering
  \tikzsetnextfilename{spearmint}
  \begin{tikzpicture}
    \begin{axis}[
      scale only axis,
      height=5cm,
      width=0.9\textwidth,
      xmin=0, xmax=90,
      ymin=0, ymax=1,
      legend entries={Unsorted, Sorted},
      legend pos=north east,
      legend style={draw=none},
      xlabel={Iteration},
      ylabel={Score}
    ]
      \addplot[mark=square*, color=color4!30!white] table [x=x, y=y, col sep=semicolon] {chapter_design/unsorted.csv};

      \addplot[
        scatter,
        color=color2,
        scatter src=explicit symbolic,
        scatter/classes={
            normal={mark=*, color2},%
            special={mark=*, colorG, draw=colorGshade}%
        }%,
        %pins near some coords={15/south:Hadamard,29/north:Divide}
      ] table [x=x, y=y, col sep=semicolon, meta=color] {chapter_design/sorted.csv}
      ;
      \node[coordinate,pin={[pin distance=1cm]above:{Hadamard}}]
        at (axis cs:16,0.55340945711058)	{};

      \node[coordinate,pin={[pin distance=.6cm]below:{Divide}}]
        at (axis cs:30,0.4252592596720848)	{};

      \node[coordinate,pin=above:{Concatenate}]
        at (axis cs:84,0.19147061365923185)	{};

    \end{axis}
  \end{tikzpicture}
\caption[Hyperparameter optimization]{Graph showing the progress of the hyperparameter optimization process}%
\label{fig:spearmint}%
\end{figure}

\begin{table}%
\centering
\begin{tabular}{ccccc}
\toprule
$p$  & $q$     & $d$ & $k$ & $l$ \\
\num{0.50} & \num{100000} & 256 & 80  & 80 \\
\bottomrule
\end{tabular}
\caption[The parameter values found to perform the best]{The parameter values found to perform the best}%
\label{tab:paramopt_goodvalues}%
\end{table}

Experiments in~\cite{node2vec} show that increasing the $r$ value (number of walks per node) improves the quality of the generated features. Knowing this, we speed up the parameter optimization by using a constant value of $r=1$, which is useful for comparing the sensitivity of the other parameters. We are aware that a higher $r$ value should be used in the final node2vec training.

Concatenating the feature vectors generally gave better results compared to using Hadamard product and division.

Since the constant $\alpha = 1$, and the value $1/p$ are much higher than $1/q$ it is most likely that the walk will remain close within the community of the node in which the walk started.

The size of the context window $k$ did not have much impact on the objective function as long as it was around a value of $80$.

\todo{Maybe comment on d and l?}
%The maximum length of the walk $l=80$
