\section{Feature Extractor}\label{feature_extractor}
The next component in the main pipeline is the feature extractor. This component generates the feature representation of a candidate link, which can then be used for classification. \todo{ref til func i kapitel 3} The feature extractor returns a single feature vector, representing the link between a given source and target article pair. As we need to classify both existing and non-existing links, it must be possible to extract feature vectors in both cases. Therefore the feature vector for a link is constructed by combining the feature vectors of the source and target articles.

As mentioned in \cref{sec:feature_generation} we aim to use feature learning, specifically network embedding, to learn these feature representations. While there are several different network embedding models, we found node2vec~\cite{node2vec} to be a suitable model. \cite{node2vec} compares node2vec to other well-known models, and finds that node2vec delivers better or equal precision for link prediction in all cases. Additionally, node2vec performs well for large networks, and offers flexibility due to tunable hyperparameters, which provides a more fine-grained control of neighborhood exploration, allowing us to test different options. This section describes how node2vec is used to generate the feature representation, along with the process of optimizing the models hyperparameters. \todo{update argumentation for node2vec choice. Flexible parameters is nice with limited knowledge of dataset + competitive performance}

\subsection{Description of node2vec} % (fold)
\label{sec:node2vec}
\todo{Should include much more theory throughout section.}
\todo{Explain: We need to find node feature vectors that we can combine to edge feature vectors.}
\todo{Explain: Similar to word embedding, where we see nodes as words, and node sequences (walks) as sentences}
Node2vec is an algorithmic framework for semi-supervised feature learning in networks~\cite{node2vec}. The goal of the algorithm is to learn a model that can reconstruct the neighborhood of a given node.


\subsubsection{Description of word2vec}
\todo{describe word2vec, including neural network etc.}
Using nodes as words and the random walks as sentences allows using word embedding algorithms on the sentences, mapping words to vectors. The node2vec reference implementation uses word2vec for word embedding. Intuitively, if two words are close two each other in the sentence, they are related. The tunable parameter $k$ specifies the size of a context window that defines the number of surrounding words that should be considered in the same context. In node2vec the context window is used for sampling neighborhoods of nodes, where it determines the size of the neighborhood.

\subsubsection{Finding Neighborhoods}
\todo{Describe the biased random walk using}
The neighborhoods of each node is constructed by performing a number of biased random walks. The walks are biased using two parameters $p$ and $q$. A high $p$ value, relative to $max(1,q)$, approximates a depth-first search, while a high $q$ value, relative to $max(1,p)$, approximates the behavior of a breath-first search. The motivation of these tunable parameters is the observation that real-world networks can be structured in many ways. The randomness and the tunable $p$ and $q$ values accounts for different types of networks, by allowing control of the exploration method.

There are three additional parameters in node2vec, $d$, $r$, and $l$, which determines the dimensions of the resulting feature vector, the number of random walks performed per node, and the maximum length of each walk, respectively.

\begin{figure}%
  \centering
  \tikzsetnextfilename{nodewalk}
  \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node] (v) {$v$};
      \node [node, fill=none, above left=of v,
              pin={[pin distance=1.2em,pin edge={very thick}]15:},
              pin={[pin distance=1.2em,pin edge={very thick}]150:},
              pin={[pin distance=1.2em,pin edge={very thick}]230:}
              ] (x1) {$x_1$};
      \node [node, fill=none, above right=of v,
              pin={[pin distance=1.2em,pin edge={very thick}]350:},
              pin={[pin distance=1.2em,pin edge={very thick}]295:},
              pin={[pin distance=1.2em,pin edge={very thick}]150:}
              ] (x2) {$x_2$};
      \node [node, fill=none, below left=of v,
              pin={[pin distance=1.2em,pin edge={very thick}]170:},
              pin={[pin distance=1.2em,pin edge={very thick}]210:}
              ] (t) {$t$};
      \node [node, fill=none, below right=of v,
              pin={[pin distance=1.2em,pin edge={very thick}]330:},
              pin={[pin distance=1.2em,pin edge={very thick}]20:}
              ] (x3) {$x_3$};
      \path [draw, very thick, inner sep=.5pt] (x1) -- node [] {$\alpha = 1$} (v);
      \path [draw, very thick, inner sep=.5pt] (x2) -- node [] {$\alpha = 1/q$} (v);
      \path [draw, very thick, swap, inner sep=.5pt] (x3) -- node [] {$\alpha = 1/q$} (v);
      \path [draw, very thick] (t) -- (x1);
      \path [draw, very thick, swap, inner sep=.5pt] (t) -- node [] {$\alpha = 1/p$} (v);
  \end{tikzpicture}
\caption[Illustration of random walk in node2vec]{Illustration of the random walk procedure in node2vec. The walk is currently in $v$ and came from $t$. It is now evaluating its next step out of node $v$. Adapted from~\cite{node2vec}.}%
\label{fig:randomwalk}%
\end{figure}

\subsubsection{Combining Feature Vectors}
By training a node2vec model on the graph, we are able to learn a function $g:V \to \mathbb{R}^d$ mapping a vertex to a vector representation of features. In order to construct a feature representation of a potential edge between two vertices, we need to combine their features. In the node2vec paper~\cite{node2vec} they propose four different binary operators $\circ : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^d$ for combining two feature vectors, with \textit{Hadamard product} yielding the best results in all of their test cases.
%We discovered that all 4 operators where commutative meaning that $a \circ b = b \circ a \ \forall a,b \in \mathbb{R}^d$ which works well in undirected graphs.
All of the four operations are commutative, which makes sense for undirected graphs, where the combined result vector does not depend on the order of the vertices. For directed graphs however the order is important.
%If we have 2 pairs of articles $(a,b) \in \Rightarrow$ and $(b,a) \in V \times V \setminus \Rightarrow$ they will have the same feature representation using any of these 4 operators, but different labels.
If we are given two pairs of articles $(a,b) \in \Rightarrow$ and $(b,a) \notin \Rightarrow$, their feature representations should intuitively be different because they have different labels.
%For this reason we will test different non-commutative operations.
Because Hadamard product produced good results in the node2vec paper we will be testing it and compare it to the corresponding non-commutative operation Hadamard division. Furthermore we choose to test concatenating the feature vectors $\mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^{2d}$, leaving it up to the classifier to interpret the combination of article features.
\todo{should we explain that hadamard is entrywise}

% \section{Overview of node2vec} % (fold)
% \label{sec:overview_of_node2vec}

% Node2vec is an algorithmic framework for semi-supervised feature learning in networks~\cite{node2vec}. The goal of the algorithm is to learn a model that can reconstruct the neighborhood of a given node.

% The neighborhoods of each node is constructed by performing a number of biased random walks. The walks are biased using two parameters $p$ and $q$. A high $p$ value, relative to $max(1,q)$, approximates a depth-first search, while a high $q$ value, relative to $max(1,p)$, approximates the behavior of a breath-first search. The motivation of these tunable parameters is the observation that real-world networks can be structured in many ways. The randomness and the tunable $p$ and $q$ values accounts for different types of networks, by allowing control of the exploration method.

% Using nodes as words and the random walks as sentences allows using word embedding algorithms on the sentences, mapping words to vectors of real numbers. The node2vec reference implementation uses word2vec for word embedding. Intuitively, if two words are close two each other in the sentence, they are related. The tunable parameter $k$ specifies the size of a context window that defines the number of surrounding words that should be considered in the same context. In node2vec the context window is used for sampling neighborhoods of nodes, where it determines the size of the neighborhood.

% There are three additional parameters in node2vec, $d$, $r$, and $l$, which determines the dimensions of the resulting feature vector, the number of random walks performed per node, and the maximum length of each walk, respectively.

% By training a node2vec model on the graph, we are able to learn feature representations for each node. However, we need to combine these features in order to construct a feature representation of a potential edge between the two nodes. We combine these two node feature vectors by using the Hadamard product, as this was found to give the best results in~\cite{node2vec}.
% section overview_of_node2vec (end)

\tikzsetnextfilename{n2vbowtie}
\begin{figure}[tbp]%
  \centering
  \input{chapter_design/n2v_bowtie_fig}

\caption[short desc]{Text. Adapted from \todo{insert source}}%
\label{fig:n2v-figure}%
\end{figure}

\subsection{Optimizing Performance / Scalability}
As also detailed in~\cite{node2vec}, node2vec is able to scale well. Every walk is independent from each other which means that many walks can take place in parallel; it is embarrassingly parallel. Our test machine had 16 CPU cores which we wanted to utilize. We therefore spawned 16 threads that each was walking the graph. This allows use to fully utilize every CPU core. As all walks have to be stored for the word2vec phase, we were concerned that the memory footprint of storing all walks in memory would be too high. We therefore dump the walks to a file, and later stream this file into memory when training word2vec. Gensim, the implemention of word2vec we use, is also able to fully utilize the CPU when training word2vec on the walks.
\todo{Find better title}

\subsection{Hyperparameter Optimization}
\todo{tilpas til nye værdier + ny metode med forskellige kombinations funktioner}
As described above, node2vec has many tunable parameters. To find the parameters that give the best set of features, we do a hyperparameter optimization pass. We first specify the parameter space that should be searched in. As this is a large space, it is not feasible to exhaust all possibilities. We therefore use the tool Spearmint~\cite{snoek2012practical} which performs Bayesian optimization by maximizing the expected improvement to efficiently search for parameters that will minimize an objective function. The objective function is $1 - \text{f-score}$, where the f-score is found by running a classifier on the node2vec model learned from the parameters. By keeping the classifier and its parameters constant, we can find the best set of parameters by finding the lowest objective function value.

To speed each iteration of the optimization up, we limit the set of start nodes to featured articles. The hyperparameter optimization is entirely autonomous and the process should be able to find good $p$ and $q$ values on its own. This means that one can learn on graph networks without knowing the underlying graph structure.

We start with a coarse search to find a good range for each parameter. This is done with large grained values for each parameter that would show local optima. After this, we search a second time with finer grained values to further refine the local optima. After completing 83 experiments, the objective function seemed to have converged by minimizing it with the parameters seen in~\cref{tab:paramopt_goodvalues}.

\begin{figure}%
\centering
\begin{tabular}{cccccc}
\toprule
$p$  & $q$     & $d$ & $r$ & $k$ & $l$ \\
0.50 & 100,000 & 256 & 1   & 80  & 80 \\
\bottomrule
\end{tabular}
\caption[The found parameter values producing a minimization]{The found parameter values producing a minimization \todo{should we write k=120?}}%
\label{tab:paramopt_goodvalues}%
\end{figure}


Since the constant $\alpha = 1$ is higher that both $1/q$ and $1/p$ it is most likely that the walk will progress close within the neighborhood of a node. Additionally since $q$ is significantly higher than $p$ there is a higher probability that a walk will return to the previous node rather than exploring outwards, which further increases the likeliness of staying in the neighborhood.

$r=1$ means that only a single random walk is performed from each node. This might be sufficient because the other parameters already increases traversal of the neighboring nodes.

The size of the context window $k$ did not have much impact on the objective function as long as it was around a value of 80. The maximum length of the walk $l=80$ \todo{comment on these values}.

