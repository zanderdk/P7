\section{Feature Extractor}\label{feature_extractor}
%Here we input the intro text.
\input{chapter_design/feature_extractor_intro}
\input{chapter_design/feature_extractor_n2v}
\input{chapter_design/feature_extractor_adjust}


\subsection{Feature Learning Support Module}
As seen in the architecture design in \cref{fig:system-overview}, the feature learning model is part of a support module. This support module consists of a parameter optimizer and a feature learner, where the parameter optimizer is responsible for identifying the hyperparameters which the feature learner will use. Here we discuss the development of these two components.

\subsubsection{Hyperparameter Optimization}\label{sec:hyperopt}
In order to produce a good feature learning model, we perform an optimization process that attempts to find the optimal set of hyperparameters for node2vec. An overview of these parameters can be seen in \cref{tab:node2vecparams}. One of them is the choice of function for combining node features, as mentioned in \cref{subsub:combining_feature_vectors}. Since the number of parameters result in a rather large search space, we use a tool called Spearmint~\cite{snoek2012practical} for the task of approaching an optimal set of hyperparameters.

Spearmint performs bayesian optimization by maximizing the expected improvement, to efficiently search for parameters that will minimize an objective function. In our case we chose the objective function $1- F_{0.5}\text{-score}$.

We obtain the $F_{0.5}\text{-score}$ for each set of hyperparameters by evaluating them using a linear support vector machine on the features generated by the feature learning model. The intuition is that a set of parameters that produces features which causes this classifier to perform well, will also generalize to other classifiers.

%As described in \cref{sec:node2vec}, node2vec has many tunable hyperparameters. An overview of these can be seen in \cref{tab:node2vecparams}. In addition to the hyperparameters we will also examine the performance of different binary operations for combining node feature vectors. To find the parameters that give the best feature representation, we do a hyperparameter optimization pass. We first specify the parameter space that should be searched in. As this is a large space, it is not feasible to exhaust all possibilities. We therefore use the tool called Spearmint~\cite{snoek2012practical}, which performs Bayesian optimization by maximizing the expected improvement, to efficiently search for parameters that will minimize an objective function. The objective function is $1 - \text{F-score}$, where the F-score is found by running a classifier on the node2vec model learned from the parameters. By keeping the classifier and its parameters constant, we can find the best set of parameters by finding the lowest objective function value.

\begin{table}[tbp]
\centering
\begin{tabular}{@{}cp{.73\textwidth}@{}}
\toprule
\textbf{Parameter} & \textbf{Description} \\
\midrule
$p$          &   Return parameter that controls the likelihood of immediately revisiting the previous node in the walk.   \\
$q$          &   Controls to which degree the walk prefers to stay in the neighborhood or to explore outwards.   \\
$d$          &   Dimensionality of the learned features.   \\
$r$          &   Number of random walks from each node.   \\
$k$          &   Size of the context window.   \\
$l$          &   Maximum length of the random walk.   \\
$\circledcirc$ & Operation used for combining node features.   \\
\bottomrule
\end{tabular}
\caption[Description of parameters in node2vec]{Description of the various parameters in node2vec}%
\label{tab:node2vecparams}
\end{table}


%\todo{Beskriv at vi optimere på et sub sæt af graphen og at det er pga speed up.}
%\todo{del mængden er featured => good og det er fordi at så behøver vi ikke lave features for all noder men kun featured og good.}
In order to increase the amount of parameter sets we can examine within our available timespan, we speed up each iteration of the optimization. This is done by limiting node2vec to only consider a subset of the graph, as well as constraining the set of nodes, that walks can start from. This subset is the set of featured and good articles, and the set of start nodes is featured articles. This limitation reduces the amount of generated features, which improves performance of the optimization process, while still making it able to walk several nodes, since this subset considers articles with reasonable linking.

%This limitation reduces the amount of generated feature representations to the featured and good articles. This improves the runtime of the optimization process, but relies on the assumption that the parameters found here will generalize to the entire data set.
%By doing this, features are only generated for featured and good articles, which greatly improves performance and still makes the algorithm able to walk several nodes, because these articles are be expected to be reasonably linked. Due to this limitation it is only possible to find features for featured and good articles, which means that we can only run parameter experiments on the subset $\{(a,b) \in P \cup N \mid \text{b is a good article}\}$ of our training data.

The parameter optimization is entirely autonomous and the process should be able to optimize the the parameters automatically. This enables learning on graph networks without knowing the underlying graph structure.

We start with a coarse search to find a good range for some parameters. This is done with large grained values for each parameter that would show local optima. After this, we search a second time with finer grained values to further refine the local optima. After completing 83 experiments, the objective function seemed to have converged, as shown on \cref{fig:spearmint}. The best parameters found are shown in~\cref{tab:paramopt_goodvalues}.

\begin{figure}%
  \centering
  \tikzsetnextfilename{spearmint}
  \input{chapter_design/spearmint_fig}
\caption[Hyperparameter optimization]{Graph showing the progress of the hyperparameter optimization process}%
\label{fig:spearmint}%
\end{figure}

\begin{table}%
\centering
\begin{tabular}{cccccc}
\toprule
$p$  & $q$ & $d$ & $k$ & $l$ & $\circledcirc$\\
\num{0.50} & \num{100000} & 256 & 80  & 80 & Concatenation \\
\bottomrule
\end{tabular}
\caption[The parameter values found to perform the best]{The parameter values that performed best}%
\label{tab:paramopt_goodvalues}%
\end{table}

Experiments in~\cite{node2vec} show that increasing the $r$ value (number of walks per node) improves the quality of the generated features. Higher values of $r$ does however also increase the execution time. Knowing this, we speed up the parameter optimization by using a constant value of $r=1$, which is useful for comparing the sensitivity of the other parameters. We are aware that a higher $r$ value should be used in the final node2vec training.

In \cref{fig:spearmint} the annotated dots on the sorted line indicates the best results of the the binary operation. It can be seen that concatenating the feature vectors generally gave better results compared to using Hadamard product and Hadamard division. The results also show that both non-commutative operations performed better than the Hadamard product, supporting the need for a non-commutative binary operation.

Since the constant $\alpha = 1$, and the value $1/p$ are much higher than $1/q$ it is most likely that the walk will remain close within the community of the node in which the walk started.
The size of the context window $k$ did not have much impact on the objective function as long as it was around a value of $80$.
%\todo{Maybe comment on d and l?}
%, because increasing means that the feature learner gets more data to learn on which intuitively would increase the performance.
By increasing $l$ every walk takes longer time because more nodes has to be visited, we chose a max $l$ value of $120$ which we found to have reasonable performance as well as running time.
%The maximum length of the walk $l=80$

\subsubsection{Feature Learner}
This component first runs a number of biased random walks for each node based on the $l$, $p$ and $q$ hyperparameters. Feature representations of each node are then learned using the best hyperparameters $d$, $k$ from the hyper parameter optimization phase. These feature representations are then passed on to the feature extractor component.

