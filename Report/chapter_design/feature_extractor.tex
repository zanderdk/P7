\section{Feature Extractor}\label{feature_extractor}
The next component in the main pipeline is the feature extractor. This component generates the feature representation of a candidate link, which can then be used for classification, as described by the function $f$ in \cref{sec:ml_def}.

The feature extractor returns a single feature vector, representing the link between a given source and target article pair. As we need to classify both existing and non-existing links, it must be possible to extract feature vectors in both cases. Therefore the feature vector for a link is constructed by combining the feature vectors of the source and target articles.

As mentioned in \cref{sec:feature_generation} we aim to use feature learning, specifically network embedding, to learn these feature representations. While there are several different network embedding models, we found node2vec~\cite{node2vec} to be a suitable model. The main advantage of node2vec is increased flexibility compared to other well-known models due to tunable hyperparameters~\cite{node2vec}, which allows us to experiment with different neighborhood exploration methods Additionally, node2vec offers highly competitive performance, and performs well for large networks~\cite{node2vec}.

This section describes how node2vec is used to generate the feature representation, along with the process of optimizing the models hyperparameters.

\subsection{Description of node2vec}
\label{sec:node2vec}
node2vec is an algorithmic framework for semi-supervised feature learning in networks~\cite{node2vec}. The goal of the algorithm is to learn feature representations for nodes in a network. To find feature representations for edges, the feature representation for two nodes can be combined. node2vec proposes a way of generating features representations for nodes based on their neighborhoods. The idea is to use word embedding, where nodes are used as words, and node sequences are used as sentences. The neighborhood of a node consists of the nodes that are close in the node sequence. In the following sections we briefly describe word embedding, and how node sequences are constructed using biased random walks in the graph. Furthermore we describe the approaches to combine node vectors presented in \cite{node2vec}, and propose a new combination method tailored for this project.

\subsubsection{Word Embedding}
Using nodes as words and node sequences generated by biased random walks as sentences allows using word embedding algorithms on the sentences, mapping words to vectors. The node2vec reference implementation uses a well-known model for word embedding called word2vec. Intuitively, two words are related if they are close to each other in a sentence. A tunable parameter $k$ specifies the size of a context window that defines the number of surrounding words that should be considered in the same context. In node2vec the context window is used for sampling neighborhoods of nodes, where it determines the size of the neighborhood.

\subsubsection{Neighborhoods}
The notion of neighborhoods is important in network embedding, as the features of a node is derived from its neighborhood. Intuitively, this implies similarity between nodes that have similar neighborhoods. The neighborhood of a given node is found by traversing the graph.

In node2vec the neighborhoods of each node $n$ is found by performing a series of biased random walks, and then observing the $k$ nearest nodes on either side of $n$ in the walks. To ensure that all nodes are visited during the random walks a number of walks, specified by the $r$ parameter, are started in every node. The maximum length of each walk is specified by the $l$ parameter.

The random walks are biased using two parameters, $p$ and $q$. An example of the biased random walk is shown in \cref{fig:randomwalk}. The walk is currently in $v$, after walking the edge $(t,v)$. Here a search bias $\alpha$ is given to each outgoing edge from $v$ according to \cref{eq:bias}, where $d_{tx}$ denotes the shortest distance from $t$ to $x$. The next edge in the walk is selected by probability sampling using the alias method.

\begin{equation}
\label{eq:bias}
\alpha_{pq}(t,x)=
\begin{cases}
  \frac{1}{p} & \text{if } d_{tx}=0 \\
  1           & \text{if } d_{tx}=1 \\
  \frac{1}{q} & \text{if } d_{tx}=2
\end{cases}
\end{equation}

By adjusting $p$ and $q$ it is possible to approximate different search strategies. A high $p$ value, relative to $max(1,q)$, approximates a depth-first search, while a high $q$ value, relative to $max(1,p)$, approximates the behavior of a breath-first search. The motivation of these tunable parameters is the observation that real-world networks can be structured in many ways. The biased random walks with the tunable $p$ and $q$ values accounts for different types of networks, by allowing control of the exploration method.

\begin{figure}%
  \centering
  \tikzsetnextfilename{nodewalk}
  \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node] (v) {$v$};
      \node [node, fill=none, above left=of v,
              pin={[pin distance=1.2em,pin edge={very thick}]15:},
              pin={[pin distance=1.2em,pin edge={very thick}]150:},
              pin={[pin distance=1.2em,pin edge={very thick}]230:}
              ] (x1) {$x_1$};
      \node [node, fill=none, above right=of v,
              pin={[pin distance=1.2em,pin edge={very thick}]350:},
              pin={[pin distance=1.2em,pin edge={very thick}]295:},
              pin={[pin distance=1.2em,pin edge={very thick}]150:}
              ] (x2) {$x_2$};
      \node [node, fill=none, below left=of v,
              pin={[pin distance=1.2em,pin edge={very thick}]170:},
              pin={[pin distance=1.2em,pin edge={very thick}]210:}
              ] (t) {$t$};
      \node [node, fill=none, below right=of v,
              pin={[pin distance=1.2em,pin edge={very thick}]330:},
              pin={[pin distance=1.2em,pin edge={very thick}]20:}
              ] (x3) {$x_3$};
      \path [draw, very thick, inner sep=.5pt] (x1) -- node [] {$\alpha = 1$} (v);
      \path [draw, very thick, inner sep=.5pt] (x2) -- node [] {$\alpha = 1/q$} (v);
      \path [draw, very thick, swap, inner sep=.5pt] (x3) -- node [] {$\alpha = 1/q$} (v);
      \path [draw, very thick] (t) -- (x1);
      \path [draw, very thick, swap, inner sep=.5pt] (t) -- node [] {$\alpha = 1/p$} (v);
  \end{tikzpicture}
\caption[Illustration of random walk in node2vec]{Illustration of the random walk procedure in node2vec. The walk is currently in $v$ and came from $t$. It is now evaluating its next step out of node $v$. Adapted from~\cite{node2vec}.}%
\label{fig:randomwalk}%
\end{figure}

\subsubsection{Combining Feature Vectors}
By training a node2vec model on the graph, we are able to learn a function $h:V \to \mathbb{R}^d$, mapping a vertex to a vector representation of features. In order to construct a feature representation of a potential edge between two vertices, we need to combine their feature vectors. In the node2vec paper~\cite{node2vec} four different binary operators $\circ : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^d$ are proposed for combining two feature vectors, with \emph{Hadamard product} yielding the best results in all tested cases.

%We discovered that all 4 operators where commutative meaning that $a \circ b = b \circ a \ \forall a,b \in \mathbb{R}^d$ which works well in undirected graphs.

All of the four operations are commutative, which is suitable for undirected graphs, where the combined result vector does not depend on the order of the vertices. For directed graphs however, the order is important.

%If we have 2 pairs of articles $(a,b) \in \Rightarrow$ and $(b,a) \in V \times V \setminus \Rightarrow$ they will have the same feature representation using any of these 4 operators, but different labels.

If we are given two article pairs $(a,b)$ and $(b,a)$ where $a \Rightarrow b$ and $b \not \Rightarrow a$, their feature representations should intuitively be different because they have different labels.

%For this reason we will test different non-commutative operations.

Because Hadamard product produced good results in the node2vec paper we will be testing it and compare it to the corresponding non-commutative operation Hadamard division. Furthermore we choose to test concatenating the feature vectors $\mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^{2d}$, leaving it up to the classifier to interpret the combination of article features.

% \section{Overview of node2vec} % (fold)
% \label{sec:overview_of_node2vec}

% Node2vec is an algorithmic framework for semi-supervised feature learning in networks~\cite{node2vec}. The goal of the algorithm is to learn a model that can reconstruct the neighborhood of a given node.

% The neighborhoods of each node is constructed by performing a number of biased random walks. The walks are biased using two parameters $p$ and $q$. A high $p$ value, relative to $max(1,q)$, approximates a depth-first search, while a high $q$ value, relative to $max(1,p)$, approximates the behavior of a breath-first search. The motivation of these tunable parameters is the observation that real-world networks can be structured in many ways. The randomness and the tunable $p$ and $q$ values accounts for different types of networks, by allowing control of the exploration method.

% Using nodes as words and the random walks as sentences allows using word embedding algorithms on the sentences, mapping words to vectors of real numbers. The node2vec reference implementation uses word2vec for word embedding. Intuitively, if two words are close two each other in the sentence, they are related. The tunable parameter $k$ specifies the size of a context window that defines the number of surrounding words that should be considered in the same context. In node2vec the context window is used for sampling neighborhoods of nodes, where it determines the size of the neighborhood.

% There are three additional parameters in node2vec, $d$, $r$, and $l$, which determines the dimensions of the resulting feature vector, the number of random walks performed per node, and the maximum length of each walk, respectively.

% By training a node2vec model on the graph, we are able to learn feature representations for each node. However, we need to combine these features in order to construct a feature representation of a potential edge between the two nodes. We combine these two node feature vectors by using the Hadamard product, as this was found to give the best results in~\cite{node2vec}.
% section overview_of_node2vec (end)

\tikzsetnextfilename{n2vbowtie}
\begin{figure}[tbp]%
  \centering
  \input{chapter_design/n2v_bowtie_fig}

\caption[short desc]{Text. Adapted from \cite{word2vec-param-learning}}%
\label{fig:n2v-figure}%
\end{figure}

\subsection{Performance \& Scalability}
As also detailed in~\cite{node2vec}, node2vec is able to scale well. Every walk is independent from each other which means that many walks can take place in parallel; it is embarrassingly parallel. Our test machine had 16 CPU cores which we wanted to utilize. We therefore spawned 16 threads that each was walking the graph. This allows use to fully utilize every CPU core.

As all walks have to be stored for the word2vec phase, we were concerned that the memory footprint of storing all walks in memory would be too high. We therefore dump the walks to a file, and later stream this file into memory when training word2vec. Gensim, the implemention of word2vec we use, is also able to fully utilize the CPU when training word2vec on the walks.

\subsection{Hyperparameter Optimization}
As described in \cref{sec:node2vec}, node2vec has many tunable hyperparameters. In addition to the hyperparameters we will also examine the performance of different binary operations for combining node feature vectors. To find the parameters that give the best feature representation, we do a hyperparameter optimization pass. We first specify the parameter space that should be searched in. As this is a large space, it is not feasible to exhaust all possibilities. We therefore use the tool Spearmint~\cite{snoek2012practical} which performs Bayesian optimization by maximizing the expected improvement to efficiently search for parameters that will minimize an objective function. The objective function is $1 - \text{f-score}$, where the f-score is found by running a classifier on the node2vec model learned from the parameters. By keeping the classifier and its parameters constant, we can find the best set of parameters by finding the lowest objective function value.

To speed each iteration of the optimization up, we limit the set of start nodes to featured articles. The parameter optimization is entirely autonomous and the process should be able to find optimize the the parameters automatically. This means that one can learn on graph networks without knowing the underlying graph structure.

We start with a coarse search to find a good range for some parameters. This is done with large grained values for each parameter that would show local optima. After this, we search a second time with finer grained values to further refine the local optima. After completing 83 experiments, the objective function seemed to have converged by minimizing it with the parameters seen in~\cref{tab:paramopt_goodvalues}. We did not optimize the $r$ parameter, but used a static value of $r=1$, as experiments in~\cite{node2vec} shows that increasing the $r$ value improves performance. This speeds up the parameter optimization, allowing us to increase the $r$ parameter when training the final node2vec model.

\begin{figure}%
\centering
\begin{tabular}{ccccc}
\toprule
$p$  & $q$     & $d$ & $k$ & $l$ \\
0.50 & 100,000 & 256 & 80  & 80 \\
\bottomrule
\end{tabular}
\caption[The found parameter values producing a minimization]{The found parameter values producing a minimization}%
\label{tab:paramopt_goodvalues}%
\end{figure}

\begin{figure}%
  \centering
  \tikzsetnextfilename{hackety-hack-hack}
  \begin{tikzpicture}
    \begin{axis}[
      scale only axis,
      height=5cm,
      width=0.9\textwidth,
      xmin=0, xmax=90,
      ymin=0, ymax=1,
      legend entries={Unsorted, Sorted},
      legend pos=north east,
      legend style={draw=none},
      xlabel={Some x label},
      ylabel={Score}
    ]
      \addplot[mark=square*, color=color4!30!white] table [x=x, y=y, col sep=semicolon] {chapter_design/unsorted.csv};
      
      \addplot[
        scatter,
        color=color2,
        scatter src=explicit symbolic,
        scatter/classes={
            normal={mark=*, color2},%
            special={mark=*, colorG, draw=colorGshade}%
        }%,
        %pins near some coords={15/south:Hadamard,29/north:Divide}
      ] table [x=x, y=y, col sep=semicolon, meta=color] {chapter_design/sorted.csv}
      ;
      \node[coordinate,pin={[pin distance=1cm]above:{Hadamard}}] 
        at (axis cs:16,0.55340945711058)	{};
        
      \node[coordinate,pin={[pin distance=.6cm]below:{Divide}}] 
        at (axis cs:30,0.4252592596720848)	{};     
        
      \node[coordinate,pin=above:{Concatenate}] 
        at (axis cs:84,0.19147061365923185)	{};
      
    \end{axis}
  \end{tikzpicture}
\caption[short desc]{Test}%
\label{fig:hacketyhackhack}%
\end{figure}

Concatenating the feature vectors generally gave better results compared to using Hadamard product and division.

Since the constant $\alpha = 1$ is higher that both $1/q$ and $1/p$ it is most likely that the walk will progress close within the neighborhood of a node. Additionally since $q$ is significantly higher than $p$ there is a higher probability that a walk will return to the previous node rather than exploring outwards, which further increases the likeliness of staying in the neighborhood.

The size of the context window $k$ did not have much impact on the objective function as long as it was around a value of 80. The maximum length of the walk $l=80$ \todo{comment on these values}.

