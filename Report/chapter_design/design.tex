\chapter{Design \& Implementation}\label{chap:design}
In this chapter we will discuss the development of the solution that was outlined in the previous chapter. The system will be explained in a top-down fashion, and as such we start off with an overview of the entire system through the architecture. Afterward (in \cref{sec:db} and onwards in this chapter), the specifics of each component will be explained in their own separate sections.

\section{Architecture}\label{sec:design_overview}
This system is built on an pipeline architecture, where the output of each primary component is fed into the next. The main benefit of this architecture is the modularity and scalability. Each component has minimal coupling with each other, by only interacting through data. As long as two components both adhere to the rules for data exchange, they can completely disregard any logic or functionality of each other. This gives the system the two advantageous properties of effortless exchange of entire components and straightforward horizontal scaling.

If system usage grows sufficiently large the data exchange could be handled by intermediate databases using, for example, a master-slave architecture where components supplying data could write to the database through one of multiple masters, and components retrieving data could read from one of multiple slaves. It is also possible to have every component run on different machines in different places. While this might not be a computational advantage, it minimizes logistics problems during scaling operations, since the system can be partially moved, component by component. Shutdown of a component can be delayed until its replacement is running. Regardless of the specific implementations, our usage of the pipeline architecture makes the system inherently modular and scalable.

\todo{Nattiya: Scalability needs to be explained more in depth}

\subsection{Limiting Input Size}
The potential input size of testing all possible combinations of articles for missing links is roughly 5 million articles squared, which totals to $2.5 \times 10^{13}$. Suppose we can process 1 million of these pairs per second, it will still take almost 290 days. An input of this size will therefore be impossible to handle during this project period. However, we may be able to heuristically limit the input size, if we assume many article pairs can be discarded immediately --- but we need to generate these pairs dynamically instead of filtering. By filtering we encounter the same problem as before, since we have the same input size of $\num{5000000}^2$. By dynamically generating the pairs, we can generate a pair of articles as needed. This way, the computationally intensive evaluation is only performed on the data that is generated as needed.

\tikzsetnextfilename{system-overview}
\begin{figure}[tb]%
  \centering
  \input{chapter_design/system-overview-fig}
\caption[Architecture diagram showing the major components of the system]{Architecture diagram showing the major components of the system. Each box is a component, and a dashed line box is a grouping of components. Arrows describe the way data flows.}%
\label{fig:system-overview}%
\end{figure}

\subsection{Main Pipeline}
The main pipeline consists of five components. The first one is a storage component, which holds all the information required to identify the link suggestions. The second component is \emph{Candidate Filtering}, which is responsible for the filtering process. As such, it extracts candidate article pairs from the database. The candidates are of the form $(A,B)$ where $A$ is a source article and $B$ is a suggested target article. The third component is the \emph{Feature Extractor}. This intermediate component generates a feature vector from a given candidate pair. As the fourth component we have the \emph{Classifier}, that is responsible for the refinement process. It does this by suggesting link that should be added to the input pair, based on the feature vector it received. Finally the \emph{Results Pool} is another storage component, which holds the suggestions until they are presented to a user through the user interface.

\subsection{Model Training}
Construction of the main pipeline includes learning a model for extracting features from article, as well as training a classifier. These tasks are performed by two support modules.

The support module for the feature learning consists of a \emph{Model Trainer} and a \emph{Parameter Optimizer}. The \emph{Parameter Optimizer} works on a smaller subset of the dataset in order to speed up the process of tuning the parameters. When an acceptable set of parameters has been found, the \emph{Model Trainer} uses these parameters to train a model on the entire dataset. The result of this is the model, which is used as the \emph{Feature Extractor}.

The other support module is for the classifier learning process. It consists of a \emph{Training Data Generator} and a \emph{Classifier Trainer}. The \emph{Training Data Generator} retrieves training pairs and labels from the dataset, and uses the \emph{Feature Extractor} to prepare the data. Then the \emph{Classifier Trainer} uses this prepared training data to create a model, which is used as our classifier.

\subsection{User Interface}

Finally, we cover the User Interface. The purpose of this component is to publish the results of the main pipeline to users who can then use the results to edit Wikipedia. Since the classifier can not be expected to reach a perfect precision, the final decision of whether to include a suggested link, should be done by a human editor. Therefore we create a user interface that allows people to retrieve the results of the main pipeline.

We consider this component separate from the main pipeline. The reason is different requirements of the two parts. While both the user interface and the main pipeline share the goals of then entire system, they have different subgoals. The main pipeline is purely focused on producing results, and the user interface only considers the delivery of those results. This difference in subgoals requires a different evaluation approach, and as such it is valuable to separate the user interface from the main pipeline.

\input{chapter_design/database}

\input{chapter_design/detailed_components_desc}

\input{chapter_design/feature_extractor}

\input{chapter_design/classifier}

\input{chapter_design/ui}
