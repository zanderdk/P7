\chapter{Design \& Implementation}\label{chap:design}
In this chapter we will discuss the development of the solution that was outlined in the previous chapter. The system will be explained in a top-down fashion, and as such we start off with an overview of the entire system through the architecture. Afterward, the specifics of each component will be explained in their own separate sections.

\section{Architecture}\label{sec:design_overview}
This system is built on an pipeline architecture, where the output of each primary component is fed into the next. The main benefit for this choice of architecture is the modularity and scalability. Each component has minimal coupling with each other, by only interacting through data. As long as two components both adhere to the rules for data exchange, they can completely disregard any logic or functionality of each other. This gives the system the two advantageous properties of effortless exchange of entire components and straightforward horizontal scaling. If system usage grows sufficiently large the data exchange could be handled by intermediate databases using, for example, a master-slave architecture where components supplying data could write to the database through one of multiple masters, and components retrieving data could read from one of multiple slaves. It is also entirely possible to have every component running on different machines in different places. While this might not be a computational advantage, it minimizes logistics problems during scaling operations, since the system can be partially moved, component by component. Shutdown of a component can be delayed until its replacement is already running. Regardless of the specific implementations, the point is still that our usage of the pipeline architecture makes the system inherently modular and scalable.

\subsection{Filtering and Refinement}
Since the potential input size of testing all possible combinations of articles for missing links is roughly 5 million articles squared, which totals to $2.5 \times 10^{13}$, we have employed a \emph{filtering and refinement} strategy. The filtering step chooses the more promising candidates based on a given policy, and the refinement step does the actual evaluation of the candidates. The intended effect of this strategy is to allow the system to produce results, even when access to computational power is limited.
\todo{Is the reasoning for filtering and refinement adequate? consider especially the final sentence}

\tikzsetnextfilename{system-overview}
\begin{figure}[tb]%
  \centering
  \input{chapter_design/system-overview-fig}
\caption[Architecture diagram showing the major components of the system]{Architecture diagram showing the major components of the system. Each box is a component, and a dashed line box is a grouping of components. Arrows describe the way data flows.}%
\label{fig:system-overview}%
\end{figure}

\subsection{Main Pipeline}
The main pipeline consists of five components. The first one is a storage component, which holds all the information required to identify the link suggestions. The second component is \emph{Candidate Filtering}, which is responsible for the filtering process. As such, it extracts candidate article pairs from the database. The candidates are of the form $(A,B)$ where $A$ is a source article and $B$ is a suggested target article. The third component is the \emph{Feature Extractor}. This intermediate component generates a feature vector from a given candidate pair. As the fourth component we have the \emph{Classifier}, that is responsible for the refinement process. It does this by suggesting link that should be added to the input pair, based on the feature vector it received. Finally the \emph{Results Pool} is another storage component, which holds the suggestions until they are evaluated through the user interface.

\subsection{Model Training}
Construction of the main pipeline includes learning a model for extracting features from article, as well as training a classifier. These tasks are performed by two support modules.

The support module for the feature learning consists of a \emph{Model Trainer} and a \emph{Parameter Optimizer}. The \emph{Parameter Optimizer} works on a smaller subset of the dataset in order to speed up the process of tuning the parameters. When an acceptable set of parameters has been found, the \emph{Model Trainer} uses these parameters to train a model on the entire dataset. The result of this is the model, which is used as the \emph{Feature Extractor}.

The other support module is for the classifier learning process. It consists of a \emph{Training Data Generator} and a \emph{Classifier Trainer}. The \emph{Training Data Generator} retrieves training pairs and labels from the dataset, and uses the \emph{Feature Extractor} to prepare the data. Then the \emph{Classifier Trainer} uses this prepared training data to create a model, which is used as our classifier.

\subsection{User Interface}

\todo{explain the user interface part in the entire system, and why it is considered a separate part from the rest}
%The user interface is not included in the main pipeline, since their requirements are considerably different. It is entirely possible that either the main pipeline or the user interface is working flawlessly, but it does show in a system evaluation, since they are interdepent.

Lorem Ipsum UI

\input{chapter_design/database}

\input{chapter_design/detailed_components_desc}

\input{chapter_design/feature_extractor}

\input{chapter_design/classifier}

\input{chapter_design/ui}
