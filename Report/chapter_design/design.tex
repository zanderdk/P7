\chapter{Design \& Implementation}\label{chap:design}
In this chapter we will introduce our final idea, which is ultimately our solution. We explain the system top-down, and as such we broadly describe the entire system before diving into the specifics.


\section{Overview}\label{sec:design_overview}
The purpose of the system is to suggest link suggestions for articles, and add these to a pool of suggestions. A user can then browse this pool for suggestions of articles to improve the outgoing links.

The system consists of a pipeline utilizing two trained models in order to make predictions. These models are
\begin{enumerate*}[label=(\roman*)]
  \item a feature extraction model, and
  \item a classifier model.
\end{enumerate*}
Both of these models are trained by temporally separate workflows. A diagram of the main pipeline and the separate workflows for training the models can be seen in \cref{fig:system-overview}. In the following sections, the main areas will be explained.

\tikzsetnextfilename{system-overview}
\begin{figure}[tb]%
  \centering
  \input{chapter_design/system-overview-fig}
\caption[Architecture diagram showing the major components of the system]{Architecture diagram showing the major components of the system. Each box is a component, and a dashed line box is a grouping of components. Arrows describe the way data flows.}%
\label{fig:system-overview}%
\end{figure}

%Throughout this chapter the different components will be explained in detail.


\subsection{Main Pipeline}
The main part of the system provides link suggestions for articles. Because it is computationally intensive to check if each of the 5.2 million articles should link to each other, we split the pipeline into a \emph{filtering} and a \emph{refinement} step. The filtering step filter out some obvious bad candidates based on some heuristic. The refinement step consists of a trained classifier.

The \emph{Article Picker} component is responsible for the filtering step. As such, it quickly extracts possible candidates from the database. The candidates are of the form $(A,B)$ where $A$ is a source article and $B$ is a suggested target article. The next component, \emph{Feature Extraction} creates a feature for each of these pairs, based on a model that has been trained ahead of time. This vector of features is then supplied to the \emph{Classifier} component, which will then make a prediction on whether or not the pair should be linked.

If a pair is predicted to have a missing link, it will be added to the pool of suggestions, which is stored in another database. This database holds all the pairs suggested for links, until a user browses these suggestions with the intention of deciding whether to add a link or not. This is the final component, the \emph{Web Interface}.


\subsection{Model Training}
Prior to the main pipeline, a way of extracting features from article must be learned, and the classifier model must be trained. This is performed by two support pipelines.

Feature learning is done by using \emph{node2vec}~\cite{node2vec}. node2vec is a framework for learning feature representations of graphs. It performs random walks between articles within the link graph. The walks are used to train a model which ultimately is used by the feature extractor. Parameter tuning is needed in node2vec, and as such we have a parameter optimization component to find optimal parameters that increases accuracy for the Wikipedia link data.

The classifier model learning pipeline uses a list of labeled article pairs to train the classifier model. The articles are gathered from the database and features are extracted using the feature extractor. The features and classification labels are passed to the classifier trainer component. The trained model is used by the classifier.

\input{chapter_design/database}

\input{chapter_design/detailed_components_desc}

\input{chapter_design/feature_learner}

\input{chapter_design/classifier}

\input{chapter_design/ui}
