\chapter{Design \& Implementation}\label{chap:design}
In this chapter we will discuss the development of the solution that was outlined in the previous chapter. The system will be explained in a top-down fashion, and as such we start off with an overview of the entire system through the architecture. Afterward, the specifics of each component will be explained in their own separate sections.

\section{Architecture}\label{sec:design_overview}
This system is built on an pipeline architecture, where the output of each primary component is fed into the next. The main benefit of this architecture is the modularity and scalability. Each component has minimal coupling with each other, by only interacting through data. As long as two components both adhere to the rules for data exchange, they can completely disregard any logic or functionality of each other. This gives the system the two advantageous properties of effortless exchange of entire components and straightforward horizontal scaling.

If system usage grows sufficiently large the data exchange could be handled by intermediate databases using, for example, a master-slave architecture where components supplying data could write to the database through one of multiple masters, and components retrieving data could read from one of multiple slaves. It is also possible to have every component run on different machines in different places. While this might not be a computational advantage, it minimizes logistics problems during scaling operations, since the system can be partially moved, component by component. Shutdown of a component can be delayed until its replacement is running. Regardless of the specific implementations, our usage of the pipeline architecture makes the system inherently modular and scalable. 

\subsection{Filtering and Refinement}
Since the potential input size of testing all possible combinations of articles for missing links is roughly 5 million articles squared, which totals to $2.5 \times 10^{13}$, we have employed a \emph{filtering and refinement} strategy. The filtering step chooses the more promising candidates based on a given policy, and the refinement step does the actual evaluation of the candidates.

\tikzsetnextfilename{system-overview}
\begin{figure}[tb]%
  \centering
  \input{chapter_design/system-overview-fig}
\caption[Architecture diagram showing the major components of the system]{Architecture diagram showing the major components of the system. Each box is a component, and a dashed line box is a grouping of components. Arrows describe the way data flows.}%
\label{fig:system-overview}%
\end{figure}

\subsection{Main Pipeline}
The main pipeline consists of five components. The first one is a storage component, which holds all the information required to identify the link suggestions. The second component is \emph{Candidate Filtering}, which is responsible for the filtering process. As such, it extracts candidate article pairs from the database. The candidates are of the form $(A,B)$ where $A$ is a source article and $B$ is a suggested target article. The third component is the \emph{Feature Extractor}. This intermediate component generates a feature vector from a given candidate pair. As the fourth component we have the \emph{Classifier}, that is responsible for the refinement process. It does this by suggesting link that should be added to the input pair, based on the feature vector it received. Finally the \emph{Results Pool} is another storage component, which holds the suggestions until they are evaluated through the user interface.

\subsection{Model Training}
Construction of the main pipeline includes learning a model for extracting features from article, as well as training a classifier. These tasks are performed by two support modules.

The support module for the feature learning consists of a \emph{Model Trainer} and a \emph{Parameter Optimizer}. The \emph{Parameter Optimizer} works on a smaller subset of the dataset in order to speed up the process of tuning the parameters. When an acceptable set of parameters has been found, the \emph{Model Trainer} uses these parameters to train a model on the entire dataset. The result of this is the model, which is used as the \emph{Feature Extractor}.

The other support module is for the classifier learning process. It consists of a \emph{Training Data Generator} and a \emph{Classifier Trainer}. The \emph{Training Data Generator} retrieves training pairs and labels from the dataset, and uses the \emph{Feature Extractor} to prepare the data. Then the \emph{Classifier Trainer} uses this prepared training data to create a model, which is used as our classifier.

\subsection{User Interface}

Finally we cover the User Interface. The purpose of this component is to publish the results of the main pipeline to users who can then edit wikipedia. Since the classifier can not be expected to reach a perfect precision, the final decision of whether to include a suggested link, should be done by a human editor. Therefore the user interface allows people to retrieve the results of the main pipeline through a RESTful web API.

The reason why this component is not considered a part of the main pipeline, is due to different requirements. While both the user interface and the main pipeline share the goals of then entire system, they have different subgoals. While the main pipeline is purely focused on producing results, the user interface considers the delivery of those results. This difference in subgoals requires a different evaluation approach, and as such it is valuable to seperate the user interface from the main pipeline.

\input{chapter_design/database}

\input{chapter_design/detailed_components_desc}

\input{chapter_design/feature_extractor}

\input{chapter_design/classifier}

\input{chapter_design/ui}
