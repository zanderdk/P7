\subsection{Node2vec Theory}\label{sec:node2vec}
The node2vec framework employs a network embedding approach to the task of learning features for nodes in a graph structure. Network embedding entails an embedding of information into each node, such that feature representations can be made. In our case, and the case of node2vec, we embed structural information.

\subsubsection{Inspiration}
The technique which node2vec uses for network embedding is built upon a similar framework called word2vec~\cite{}. Word2vec does word embedding in text, and like network embedding this is a way of embedding each word with information. In order to better understand the reasoning behind node2vec, it is beneficial to take the example of word2vec.

The core idea behind word2vec is that every word can be characterized by its relation to other words. An example could be a text where the words \enquote{king} and \enquote{queen} are boths equally related to the word \enquote{regant}, but \enquote{king} have a closer relation to the word \enquote{man}, and \enquote{queen} have a similar relation to the word \enquote{woman}.\todo{overvej at udvide eksempelt med vector operations forklaring af regent + woman = queen}

The way these relations are found is by considering which words occur in the same sentences. Word2vec goes through every sentence in a body of text and aggregates the analysis of the word relations, such that information on every word is gathered.

Node2vec employs the same basic approach to nodes, in that each node is characterized by its relation to other nodes, both the direct relations seen as edges but also indrect relations across multiple edges. But since there is no predifined replacement for sentences in the context of graph analysis, node2vec creates the graphs \enquote{sentences}.

In the same way that a sentence is a sequence of words, the setences of the graphs are sequences of nodes. These sequence are generated through multiple biased random walks throughout the graph.

%\subsubsection{Word Embedding}
%Using nodes as words and node sequences generated by biased random walks as sentences allows the use of word embedding algorithms on the sentences, mapping words to vectors. The node2vec reference implementation uses a well-known model for word embedding called word2vec. Intuitively, two words are related if they are close to each other in a sentence. A parameter $k$ specifies the size of a context window that defines the number of surrounding words that should be considered in the same context. In node2vec the context window is used for sampling neighborhoods of nodes, where it determines the size of the neighborhood.

\subsubsection{Biased Random Walks}
Unlike textual analysis, there are no explicitly stated sequences for nodes in a graph. In a body of text, sentences are expect to be syntactically and semantically well formed, at least to some extend, but how can a graph equivalent be constructed?

This is done through biased random walks. By walking randomly through a graph multiple times, we can get a probabilistic view of which nodes tend to occur in the same sequences.\todo{complete this argument for random walks}

The random walks are made biased because real-world networks are structered in different ways. Biasing the random walks in different ways can account for different types of networks, by directing the exploration model toward finding the most significant structures in the graph. The random walks are biased by making it more likely to follow certain edges in the graph. This is done through two parameters, $p$ and $q$. An example of the biased random walk is shown in \cref{fig:randomwalk}. In this illustration the walk is currently in $v$, after walking the edge $(t,v)$. Here a search bias $\alpha$ is given to each outgoing edge from $v$ according to \cref{eq:bias}, where $d_{tx}$ denotes the shortest distance from $t$ to $x$. The next edge in the walk is selected by probability sampling using the alias method.

\begin{figure}%
  \centering
  \tikzsetnextfilename{nodewalk}
  \input{chapter_design/random_walks_fig}
\caption[Illustration of random walk in node2vec]{Illustration of the random walk procedure in node2vec. The walk is currently in $v$ and came from $t$. It is now evaluating its next step out of node $v$. Adapted from~\cite{node2vec}.}%
\label{fig:randomwalk}%
\end{figure}

\begin{equation}
\label{eq:bias}
\alpha_{pq}(t,x)=
\begin{cases}
  \frac{1}{p} & \text{if } d_{tx}=0 \\
  1           & \text{if } d_{tx}=1 \\
  \frac{1}{q} & \text{if } d_{tx}=2
\end{cases}
\end{equation}

%%%%%% YES, THERE IS A D in BREADTH. IT IS NOT SPELLED BREATH-FIRST SEARCH!  %%%%%%%%
By adjusting $p$ and $q$ it is possible to approximate different search strategies. A high $p$ value, relative to $\max(1,q)$, approximates a depth-first search, while a high $q$ value, relative to $\max(1,p)$, approximates the behavior of a breadth-first search. According to \cite{node2vec} an approximation of depth-first search will result in more structural information across the entire graph, where an approximation of a breadth-first search will yield information on communities in the graph.
%%%%%% YES, THERE IS A D in BREADTH. IT IS NOT SPELLED BREATH-FIRST SEARCH!  %%%%%%%%

\todo{we kinda skip the part where we analyse the random walks. This might be okay, but we should probably talk about it}