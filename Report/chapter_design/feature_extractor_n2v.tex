\subsection{node2vec Overview}\label{sec:node2vec}
The node2vec framework employs a network embedding approach to the task of learning features for nodes in a graph structure. Network embedding entails an embedding of information into each node, such that feature representations can be derived. In our case, and the case of node2vec, we embed structural information.

\subsubsection{Inspiration}
The technique which node2vec uses for network embedding is built upon a similar framework called word2vec~\cite{word2vec}. word2vec performs word embedding in text, and similarly to network embedding this is a way of embedding each word with information. We motivate the use of node2vec by introducing a typical word2vec example, to illustrate the concept behind embedding techniques.

The core idea behind word2vec is that every word can be characterized by its relation to other words. An example could be a text where the words \enquote{king} and \enquote{queen} are both equally related to the word \enquote{regant}, but \enquote{king} has a closer relation to the word \enquote{man}, and \enquote{queen} has a similar relation to the word \enquote{woman}.

These relations are discovered by considering which words occur in the same sentences. word2vec goes through every sentence in a body of text and aggregates the analysis of the word relations, such that information on every word is gathered.\todo{se p√• skip-gram/forklar hvorfor ikke bag of words}

node2vec employs the same approach to nodes, in that each node is characterized by its relation to other nodes, both the direct relations seen as edges but also indirect relations across multiple edges.
%But since there is no predifined replacement for sentences in the context of graph analysis, node2vec creates a graph equivalent.
In the same way that word2vec considers sequences of words, i.e.\ sentences, node2vec considers sequences of nodes. These sequences are generated by performing multiple biased random walks throughout the graph.

%\subsubsection{Word Embedding}
%Using nodes as words and node sequences generated by biased random walks as sentences allows the use of word embedding algorithms on the sentences, mapping words to vectors. The node2vec reference implementation uses a well-known model for word embedding called word2vec. Intuitively, two words are related if they are close to each other in a sentence. A parameter $k$ specifies the size of a context window that defines the number of surrounding words that should be considered in the same context. In node2vec the context window is used for sampling neighborhoods of nodes, where it determines the size of the neighborhood.

\subsubsection{Biased Random Walks}
In textual analysis the ordering of words in a sentence provides structure, whereas nodes in a graph does not have any equivalent ordering. node2vec derives the structure of a graph through biased random walks. By walking randomly through a graph multiple times, we can get a probabilistic view of which nodes tend to occur in the same sequences.\todo{complete this argument for random walks}

Biasing the random walks in different ways can account for different types of graph structures, by influencing the walks to represent the most significant structures in the graph. The random walks are biased by adjusting the probability of following certain types of edges in the graph. This is done through two parameters, $p$ and $q$. An example of the biased random walk is shown in \cref{fig:randomwalk}. In this illustration the walk is currently in $v$, after walking the edge $(t,v)$. Here a search bias $\alpha$ is given to each outgoing edge from $v$ according to \cref{eq:bias}, where $d_{tx}$ denotes the shortest distance from $t$ to $x$. The next edge in the walk is selected through probability sampling using the alias method~\cite{alias-method}.

\begin{figure}%
  \centering
  \tikzsetnextfilename{nodewalk}
  \input{chapter_design/random_walks_fig}
\caption[Illustration of random walk in node2vec]{Illustration of the random walk procedure in node2vec. The walk is currently in $v$ and came from $t$. It is now evaluating its next step out of node $v$. Adapted from~\cite{node2vec}.}%
\label{fig:randomwalk}%
\end{figure}

\begin{equation}
\label{eq:bias}
\alpha_{pq}(t,x)=
\begin{cases}
  \frac{1}{p} & \text{if } d_{tx}=0 \\
  1           & \text{if } d_{tx}=1 \\
  \frac{1}{q} & \text{if } d_{tx}=2
\end{cases}
\end{equation}

%%%%%% YES, THERE IS A D in BREADTH. IT IS NOT SPELLED BREATH-FIRST SEARCH!  %%%%%%%%
By adjusting $p$ and $q$ it is possible to approximate different search strategies. A high $p$ value, relative to $\max(1,q)$, approximates a depth-first search, while a high $q$ value, relative to $\max(1,p)$, approximates the behavior of a breadth-first search. According to \cite{node2vec} an approximation of depth-first search will result in more structural information across the entire graph, where an approximation of a breadth-first search will yield information on communities in the graph.
%%%%%% YES, THERE IS A D in BREADTH. IT IS NOT SPELLED BREATH-FIRST SEARCH!  %%%%%%%%
Once an adequate number of walks have been collected, the word2vec framework is used to analyze them, and produces the feature representations of the nodes in the graph.