\section{Database}\label{sec:db}
Our approach to identifying missing links requires readily available data, which we facilitate by storing the required data in a local database. This database is the first component in the main pipeline seen in \cref{fig:system-overview}. 

\subsection{Database Design}\label{sec:db_design}
Because of the need to store a graph as mentioned in \cref{sec:choice_of_graph}, a natural choice is to use a native graph database. We use Neo4j~\cite{neo4j} as it performs well for graph based queries, and supports extensions to its query language through Java plugins. Neo4j scales gracefully~\cite{neo4jscale} and servers can be upgraded or added until performance is satisfactory. The data model in Neo4j is based on nodes, relationships between nodes, and properties on these. Each node and relationship can be annotated with labels to distinguish between types when querying the database.

We store Wikipedia articles as nodes with their title as a property, and links between articles as relationships. Labels are used to distinguish between types of articles and links.

\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}

\begin{table}[tbp]
  \centering
    \begin{tabular}{@{}p{.24\textwidth}p{.50\textwidth}R{.15\textwidth}@{}}
      \toprule
      \textbf{Label} & \textbf{Description} & \textbf{Count} \\
      %\midrule
      \mono{Article} & A Wikipedia article & \num{11159213} \\
      \mono{FeaturedArticle} & A Wikipedia article marked as \emph{Featured} & \num{4820} \\
      \mono{GoodArticle} & A Wikipedia article marked as \emph{Good} & \num{23741}\\
      \mono{RedirectPage} & A redirecting Wikipedia page &\num{7013417} \\
      \midrule
      & \multicolumn{1}{r}{\emph{Total number of nodes}} & \num{18172630} \\
      \bottomrule
    \end{tabular}
    \caption[Node labels in the database]{Node labels in the database. Note that some nodes have multiple labels.}%
    \label{tab:db_labels_nodes}
\end{table}
\begin{table}[tbp]
    \centering
    \begin{tabular}{@{}p{.24\textwidth}p{.50\textwidth}R{.15\textwidth}@{}}
      \toprule
      \textbf{Label} & \textbf{Description} & \textbf{Count} \\
      %\midrule
      \mono{LinksTo} & A link between two articles & \num{138422339} \\
      \mono{TrainingData} & A link that can be used during training & \num{294857} \\
      \mono{TestData} & A link that is used only during testing & \num{147429} \\
      \mono{RedirectsTo} & An edge describing a redirect & \num{7013417} \\
      \midrule
      & \multicolumn{1}{r}{\emph{Total number of relationships}} & \num{145878042} \\
      \bottomrule
    \end{tabular}
    \caption[Relationship labels in the database]{Relationship labels in the database}%
    \label{tab:db_labels_edges}
\end{table}

\subsection{Populating the Database}\label{sec:db_populate}
%Because Wikipedia requests that bots are not used to crawl the articles~\cite{wiki-bots}, we instead use a readily available dataset from DBpedia~\cite{dbpedia}. This dataset is just a dump of all Wikipedia \emph{pages}. The pages consists of for example articles, user pages, and talk pages. Since we are only interested in articles, we prune non-article pages based on the namespace prefixes used on Wikipedia.
Because Wikipedia requests that bots are not used to crawl the articles~\cite{wiki-bots}, we instead use readily available datasets\footnote{Data extracted from Wikipedia dumps in October 2015} from DBpedia~\cite{dbpedia}. The datasets used are a list of all page links and a list of page redirects. The pages consists of articles as well as pages for users, talks, files, etc. Since we are only interested in articles, we prune non-article pages based on the namespace prefixes used on Wikipedia.

Redirects are responsible for redirecting synonyms and common misspellings to the main article. For example, trying to access the article on ``Santa'' will redirect to ``Santa Claus''. The redirects are necessary to include since they are referred to by many links. We do not handle these redirects when creating the database. Instead we handle them by following them as needed when we later traverse the graph.

%To add relationships between the article nodes in the the database, we could have parsed the text in each article and created edges based on all encountered links. However, DBPedia have created a separate dataset containing links in an accessible tuple format:
%\begin{center}
%\mono{\emph{<source\_page> <type> <target\_page>}}
%\end{center}

%We therefore use this dataset instead of manually extracting the links.
A node is created in the database for each Wikipedia article and redirect page in the datasets, as seen in \cref{tab:db_labels_nodes}.
The nodes are connected by adding relationships for the links and redirects, with the labels seen in \cref{tab:db_labels_edges}.

%Similar to featured articles, Wikipedia also has \emph{good articles} that follow most of the guidelines, providing a satisfactory overall quality~\cite{wiki-good-articles}. However, there is no requirement for them to follow the guidelines for proper linking. We use the set of good articles later in hyperparameter optimization (\cref{sec:hyperopt}).

While importing the articles and links into the database, additional labels \mono{FeaturedArticle} and \mono{GoodArticle} are added to \emph{featured} and \emph{good} articles respectively. A \emph{good} article~\cite{wiki-good-articles} meets the core set of editorial standards on Wikipedia, providing a satisfactory overall quality. They do not meet all of the criteria of a featured article such as the guidelines for appropriate linking. We will be using them for hyperparameter optimization in \cref{sec:hyperopt}.

As seen in \cref{tab:db_labels_edges}, the links are split into three groups, \mono{LinksTo}, \mono{TrainingData}, and \mono{TestData}. \mono{TrainingData} and \mono{TestData} are disjoint links from featured articles that will later be used to train and evaluate the classifier. \mono{LinksTo} are all of the remaining links. The splits are made randomly, according to a partitioning given in \cref{sec:classifier}.

%page_links_unredirected_en.ttl.bz2
%redirects.en.ttl.bz2

Out of all articles, 0.04\% are featured, and 0.32\% of all links are links from featured articles. These are low proportions and might not generalize the entire graph. As described in \cref{sec:choice_of_graph}, links from featured articles is our ground truth for appropriate linking, and for that reason we want to learn the structure of these links even though they comprise a smaller proportion of the data.

\todo{Comment on why we have 11 million articles instead of 5 million.}