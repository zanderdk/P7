\section{Classifier}\label{sec:classifier}
As described in \cref{sec:machine_learning_task} we want to solve a supervised binary classification problem. The following section describes the classifier component. Given a classifier model and a vector of features representing a pair of articles A and B, the component should decide whether a link should exist between articles A and B. \todo{man kan måske binde komponent beskrivelsen sammen med andre afsnit ved at skriv blabla, nu har vi node2vec aka features, nu skal vi bruge de features}

\subsection{Training Data}\label{sec:training_data}
Like all supervised learning, the classifier model is learned through training data.

As described in \ref{sec:choice_of_graph} we will be using featured articles and what they link to, to train the classifier. This data is split into 20\% test data and 80\% training data. This specific split is made to preserve the link structure in the graph while still having a reasonable amount to test on. To train the classifier to find links that are not already present, a random distribution of 50\% of the edges is removed from the training data. \todo{Er det her helt præcist nok? Vi fjerner vel 50\% fra training data for at undgå at vi overfitter når vi laver walks i node2vec. Ville det være bedre at skrive at de 50\% er til at konstruere en node2vec model ( node2vec kan gå i LINKSTO)?}
%One sample of training data has the form \mono{<source article> <target article> <label>}. The source and targeet article are titles of pairs of articles. The label on the training samples can either be positive (link) or negative (do not link).

As described in \cref{sec:machine_learning_task} we use a set of positive labeled training pairs $P$ and a set of negative labeled training pairs $N$. The positive training samples are pairs of linked articles as defined by $P$, whereas the negative training pairs are a subset of $N$ randomly sampled from the training data. \todo{Hvad er forskellen på $P$ og the positive training samples? specificer}

%The positive samples are pairs of articles $(a,b)$ such that article $a$ is featured and there exists a link from article $a$ to $b$.



%We first experimented with negative samples being random article pairs $(a,b)$ fulfilling the condition of $N$: that article $a$ is featured and there does not exist a link from article $a$ to $b$. The problem with this approach is that it is too easy to overfit using these negative samples. As the article pairs are randomly sampled, most of the times, there is very little relatedness between the two articles. This is of course the point of a negative training sample, but because most of the negative samples were so unrelated, the classifiers used in our experiments could too easily differentiate positives from negatives. We needed another method that would give us negative samples where the articles have a higher likelihood of being related. Our second approach was to extract N-grams from all featured articles. Iterating through all N-grams for a featured article, a negative sample would be generated if another article had the exact same title as the N-gram, but the two article were not linked.

Notice that all samples only establishes ground truth from featured articles. This is based on our assumption that featured articles are correctly linked.\todo{behøver vi at nævne det her igen?} 


\todo{Det her subafsnit skal egentlig bare beskrive hvad vi bruger af positive og negative samples, og hvorfor vi laver 40-40-20 splittet. Hvis det er der, på en god måde, er det fint.}

\subsection{Choosing a Classifier}\label{choosing_classifier}
To get a coarse estimate of which classifier seems the most appropriate for our purpose, we create a test harness that will run a range of classification algorithms on the same dataset. By comparing the results of the different classifiers, we will choose one to further optimize. \todo{beskriv at vi laver en bred sammenligning af måske 20 algoritmer. Den liste får vi så reduceret til 4-5 stykker, som alle ser ud til at have samme performance. Vi kan eventuelt compare disse 4 på forskellige f0, f1, f0.5 measure i samme graf. Argumenter herefter at det er svært at vide hvilken af disse 4 der har størst potentiale at optimere videre på. Vi vælger dog 1 at optimere videre på. Vi er klar over at vi burde optimere alle, men det har vi ikke ressourcerne til.}.

The estimation will be based on 3-fold cross-validation \todo{kan vi ikke droppe forklaring i hvad cross validation er?}, in which each sample is split into 3 equally sized subsamples. F-scores are computed with each subsample used once as validation data and the remaining as training data. The average of the scores is the classifiers estimated accuracy\todo{lad være med at bruge accuracy, bedre at definere hvilken metric vi bruger. Når dette skrives, så husk på at test/evaluation allerede har noget tekst omkring precision vs recall, så overvej om det skal flyttes hertil. Lav måske et evaluation metrics sub afsnit her, og referer hertil i test/evaluation afsnittet.}.

The examined algorithms was chosen mostly based on classifiers available in the scikit-learn python library. \todo{better reasons? stort udvalg med samme interface, så vi kan prøve mange af}

\todo{Indsæt tabel med classifier resultater, måske barchart, og kommenter resultater}


% \begin{table}[tbp]
% \centering
% \begin{tabular}{@{}lp{.75\textwidth}@{}}
% \toprule
% \textbf{Classifier} & \textbf{Result} \\
% \midrule
% Baseline           &  DummyClassifier()                \\
% Nearest Neighbors  &  KNeighborsClassifier(3)          \\
% Linear SVM         &  SVC(kernel="linear", C=0.025)    \\
% RBF SVM            &  SVC(gamma=2, C=1)                \\
% Gaussian Process   &  GaussianProcessClassifier        \\
% Decision Tree      &  DecisionTreeClassifier           \\
% Random Forest      &  RandomForestClassifier           \\
% Neural Net         &  MLPClassifier, KerasClassifier   \\
% AdaBoost           &  AdaBoostClassifier()             \\
% Naive Bayes        &  GaussianNB()                     \\
% QDA                &  QuadraticDiscriminantAnalysis()  \\
% \bottomrule
% \end{tabular}
% \caption[Classifiers]{Classifiers}%
% \label{tab:classifiers}
% \end{table}

\todo{indsæt analyse af resultater.}


\todo{mangler afsnit om hyper parameter optimization af den fundne classifier. Motiver hvorfor vi gør det, og hvordan vi gør, lignende node2vec optimization. Præsenter også resultater for, hvad der virkede og hvor godt det var}

\todo{hvordan laver vi segway over mod ui afsnit?}
