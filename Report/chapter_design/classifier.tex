\section{Classifier}\label{sec:classifier}
As described in \cref{sec:machine_learning_task} we want to solve a supervised binary classification problem. This component is solving this problem. That is, given a classifier model and a vector of features representing a pair of articles A and B, it should decide whether a link should exist between article A and B.

\subsection{Training Data}\label{sec:training_data}
Like all supervised learning, the classifier model is learned through training data.

As described in \ref{sec:choice_of_graph} we will be using featured articles and what they link to train the classifier. This data is split into 20\% test data and 80\% training data. This is done to preserve link structure while still having a reasonable amount to test on. To train the classifier to find links that are not already present, a random distribution of 50\% of the edges is removed from the training data.
%One sample of training data has the form \mono{<source article> <target article> <label>}. The source and targeet article are titles of pairs of articles. The label on the training samples can either be positive (link) or negative (do not link).

As described in \cref{sec:machine_learning_task} we use a set of positive labeled training pairs $P$ and set of negative labeled training pairs $N$.

%The positive samples are pairs of articles $(a,b)$ such that article $a$ is featured and there exists a link from article $a$ to $b$.

The positive training samples are pairs of linked articles as defined by $P$, whereas the negative training pairs are a subset of $N$ randomly sampled from the training data.

%We first experimented with negative samples being random article pairs $(a,b)$ fulfilling the condition of $N$: that article $a$ is featured and there does not exist a link from article $a$ to $b$. The problem with this approach is that it is too easy to overfit using these negative samples. As the article pairs are randomly sampled, most of the times, there is very little relatedness between the two articles. This is of course the point of a negative training sample, but because most of the negative samples were so unrelated, the classifiers used in our experiments could too easily differentiate positives from negatives. We needed another method that would give us negative samples where the articles have a higher likelihood of being related. Our second approach was to extract N-grams from all featured articles. Iterating through all N-grams for a featured article, a negative sample would be generated if another article had the exact same title as the N-gram, but the two article were not linked.

Notice that all samples only establishes ground truth from featured articles. This is based on our assumption that featured articles are correctly linked.

\subsection{Choosing a Classifier}\label{choosing_classifier}
We choose a classifier by comparing a range of different algorithms. To get a coarse estimate of which classifier seems the most appropriate for our purpose, we create a test harness that will sequentially run all the classification algorithms on the same dataset. By comparing the results of the different classifiers, we will choose one to further optimize.

The estimation will be based on 3-fold cross-validation, in which each sample is split into 3 equally sized subsamples. F-scores are computed with each subsample used once as validation data and the remaining as training data. The average of the scores is the classifiers estimated accuracy.

The examined algorithms was chosen mostly based on classifiers available in the scikit-learn python library. \todo{better reasons?}

\todo{Inds√¶t tabel med classifier resultater, og kommenter resultater}


% \begin{table}[tbp]
% \centering
% \begin{tabular}{@{}lp{.75\textwidth}@{}}
% \toprule
% \textbf{Classifier} & \textbf{Result} \\
% \midrule
% Baseline           &  DummyClassifier()                \\
% Nearest Neighbors  &  KNeighborsClassifier(3)          \\
% Linear SVM         &  SVC(kernel="linear", C=0.025)    \\
% RBF SVM            &  SVC(gamma=2, C=1)                \\
% Gaussian Process   &  GaussianProcessClassifier        \\
% Decision Tree      &  DecisionTreeClassifier           \\
% Random Forest      &  RandomForestClassifier           \\
% Neural Net         &  MLPClassifier, KerasClassifier   \\
% AdaBoost           &  AdaBoostClassifier()             \\
% Naive Bayes        &  GaussianNB()                     \\
% QDA                &  QuadraticDiscriminantAnalysis()  \\
% \bottomrule
% \end{tabular}
% \caption[Classifiers]{Classifiers}%
% \label{tab:classifiers}
% \end{table}

