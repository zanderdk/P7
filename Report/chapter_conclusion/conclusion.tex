\chapter{Conclusion}\label{chap:conclusion}
This project addressed the problem of supplying reasonable suggestions of links, that should be added to Wikipedia articles. Wikipedia contains a large amount of articles, and each day articles are created and existing are edited. To ensure the high quality of Wikipedia, relationships (links) between articles have to be maintained. Otherwise, the easy way of navigating Wikipedia by links in articles would be impaired. Quality control is required for this task, but this is a daunting task for any single human editor. As such, Wikipedia employs a community based approach. 

We present a method of automating this process. The system is designed and implemented with scalability in mind. It automatically finds suggestions of articles that should be linked, and presents these to a user. The user can then decide whether or not the suggestion was appropriate. As such, we leave the ultimate decision in the hands of the user, since we recognize that our system has not been tested to the extent, that allows us to say with confidence that it works autonomously in most cases.

In the problem statement, we formulated the following questions:
\begin{itemize}
	\item List from introduction...
\end{itemize}

\todo{How we answered these. One paragraph for each point.}

\section{Future Work}
The following section outlines components that we think can be improved, but we did not manage to implement in the alloted project time.

\subsection{Candidate Generator}
To be able to scale the system for more users, an adequate number of link suggestions has to be available for users to consider. One way to increase the number of suggestions is by improving the quality of the candidate pairs generated by the candidate generator. It would be useful to examine methods for increasing the likelihood that relevant candidates are generated, while still keeping the amount of irrelevant candidates down to a manageable level.

A possible solution would be to supplement or replace the existing clickstream and n-gram approach with one that ranks all article pairs based on similarity. The similarity score could be derived from the underlying word2vec model in node2vec.

\subsection{Training Data}
As described in \todo{ref til test/eval hvor vi outliner hvad der gik galt} we believe that the generality of the classifier can be improved by improving the training data.

Because the negatives are selected at random, the training data contains very few cases, where articles have high relevance but still should not be linked. Extending the data with such samples could improve performance of the classifier. Additionally it might be useful to extend the data with training samples for linking in the opposite direction, since we do not know to which extend the direction between pairs of articles is being considered. Furthermore the number of samples in the training data could be increased to allow the classifier to generalize better on the whole of Wikipedia. One way to increase the amount of training data would be to also consider links from good articles as positives, even though we cannot assume them to have good linking.

% non-featured
Both the training and test data does not take non-featured articles into account. This could be a problem since featured articles might not be a good representation of normal articles, leading to generality problems. In the future it would be relevant to examine how the model can be trained to evaluate article pairs that are not featured. One way might be to analyse the edit history of articles, which is discussed in the following section. %Other approaches could be to use a n-gram or clickstream data as mentioned in \todo{ref}.

As mentioned in \cref{sec:db} we currently have \num{11159213} articles in our dataset. The majority of these articles are automatically generated pages with less or no content. Pruning these articles from the data would probably improve the feature model, because it could avoid traversing these during graph walks.

\subsection{Real-time Data}
We only considered Wikipedia data from 2015. A future improvement would be to consider the live version of Wikipedia, to be able to suggest links that have not already been inserted.
By working on real-time Wikipedia data, the system could be improved to suggest links for recently edited and added articles. This would be beneficial for editors, since new articles are likely to be missing links.

\subsection{Node2vec}
%node2vec
\todo{node2vec stuff}

\subsection{Reinforcement Learning}
The current system UI is engineered to accept feedback on the suggested links, but the feedback is currently not used. Reinforcement learning techniques could be used to improve the performance of the system by learning on the given user feedback.

\subsection{User Interface}
\todo{vil vi have dette med? vi kan skrive at vi skal lave en brugeranalyse for at finde ud af mangler/hvad de gerne vil have}

\subsection{Precise Link Location Suggestions}
The current system operates in a binary mode; to link or not to link. To further assist the users, and speed up the review process, the system could suggest where the link should be placed in the article. This improvement would need to combine the structural analysis this report is concerned about with semantic analysis.

\subsection{Automatic Link Insertion}
Once the system is able to suggest precise link locations, an improvement would be to automatically insert suggested links in articles once they are marked by users to be appropriate. This would significantly would decrease the workload required by editors.
