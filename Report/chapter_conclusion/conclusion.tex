\chapter{Conclusion}\label{chap:conclusion}
This project addressed the problem of supplying reasonable suggestions of links, that should be added to Wikipedia articles. Wikipedia contains a large amount of articles, and each day articles are created and existing are edited. To ensure the high quality of Wikipedia, relationships (links) between articles have to be maintained. Otherwise, the easy way of navigating Wikipedia by links in articles would be impaired. Quality control is required for this task, but this is a daunting task for any single human editor. As such, Wikipedia employs a community based approach. \todo{Denne paragraf står allerede i introduction? Kan vi ikke bare undlade det og så komme direkte til de vigtige konklusioner?}

We present a method of automating this process. The system is designed and implemented with scalability in mind. It automatically finds suggestions of articles that should be linked, and presents these to a user. The user can then decide whether or not the suggestion was appropriate. As such, we leave the ultimate decision in the hands of the user, since we recognize that our system has not been tested to the extent, that allows us to say with confidence that it works autonomously in most cases.

In the problem statement, we formulated the following questions:

\subproblems

% • How can Wikipedia articles be modeled as a graph, such that information relevant for the linking problem is captured?
We modeled Wikipedia as a directed graph with articles as nodes and links as edges. As described in \cref{sec:db} we used DBpedia as a resource of obtaining the article and linking data, and used the native graph database Neo4j to store and manage this data as a graph. Based on this graph, we used a feature learning technique called network embedding to generate a set of features from each article. This was achieved by utilizing the algorithmic framework node2vec to autonomously explore and generate article features based on random walks performed in the graph. Because the features are based on graph walks, they capture certain aspects of the linking structure between articles. Specifically, the node2vec model was optimized to predict links from featured articles by combining these features. As described in \cref{sec:hyperopt}, this led to a configuration of hyperparameters that among other things puts weight on exploring close neighborhoods of nodes. After hyperparameter optimization the generated feature model was used to produce features from pairs of articles, for a classifier trained to suggest links.

% • How can machine learning be used to suggest potential articles links, that would improve article quality?

% • How can editors be presented with suggestions of links that could be added to Wikipedia articles?

\todo{How we answered these. One paragraph for each point.}

\section{Future Work}\label{sec:future_work}
The following section outlines components that we think can be improved, but we did not manage to implement in the allotted project time.

\subsection{Candidate Generator}
To be able to scale the system for more users, an adequate number of link suggestions has to be available for users to consider. One way to increase the number of suggestions is by improving the quality of the candidate pairs generated by the candidate generator. It would be useful to examine methods for increasing the likelihood that relevant candidates are generated, while still keeping the amount of irrelevant candidates down to a manageable level.

A possible solution would be to supplement or replace the existing clickstream and n-gram approach with one that ranks all article pairs based on similarity. The similarity score could be derived from the underlying word2vec model in node2vec.

\subsection{Training Data}
As described in \todo{ref til test/eval hvor vi outliner hvad der gik galt} we believe that the generality of the classifier can be improved by improving the training data.

Because the negatives are selected at random, the training data contains very few cases, where articles have high relevance but still should not be linked. Extending the data with such samples could improve performance of the classifier. Additionally it might be useful to extend the data with training samples for linking in the opposite direction, since we do not know to which extend the direction between pairs of articles is being considered. Furthermore the number of samples in the training data could be increased to allow the classifier to generalize better on the whole of Wikipedia. One way to increase the amount of training data would be to also consider links from good articles as positives, even though we cannot assume them to have appropriate linking.

% non-featured
Both the training and test data does not take non-featured articles into account. This could be a problem since featured articles might not be a good representation of normal articles, leading to generality problems. In the future it would be relevant to examine how the model can be trained to evaluate article pairs that are not featured. One way might be to analyse the edit history of articles, which is discussed in the following section. %Other approaches could be to use a n-gram or clickstream data as mentioned in \todo{ref}.

As mentioned in \cref{sec:db} we currently have \num{11159213} articles in our dataset. The majority of these articles are automatically generated pages with little or no content. Pruning these articles from the dataset will probably improve the feature model, because it could avoid traversing these during graph walks.

\subsection{Real-time Data}
We only considered Wikipedia data from 2015. A future improvement would be to consider the live version of Wikipedia, to be able to suggest links that have not already been inserted.
By working on real-time Wikipedia data, the system could be improved to suggest links for recently edited and added articles. This would be beneficial for editors, since new articles are likely to be missing links.

\subsection{Reinforcement Learning}
The current system UI is engineered to accept feedback on the suggested links, but the feedback is currently not used. Reinforcement learning techniques could be used to improve the performance of the system by learning on the given user feedback.

\subsection{User Interface}
To fully know how best to improve the user interface would require a usability analysis of the current implementation. We can however come up with ideas for further development based on needs envisioned by us.

We believe the following two API endpoints would be useful:
\begin{itemize}
    \item Given articles A and B as parameters, returns whether A should link to B.
    \item Given article A, returns a list of articles that A should link to.
\end{itemize}

Given the user interface is the facade to the users, it makes sense to also develop a graphical user interface based on the provided API that allows users to interact with the system in an intuitive fashion.
% endpoint: (a,b) -> skal der være link mellem a og b?
% endpoint: a -> giv mig artikler som der kan linkes til fra a
% add gui

\subsection{Precise Link Location Suggestions}
The current system operates in a binary mode; to link or not to link. To further assist the users, and speed up the review process, the system could suggest where the link should be placed in the article. This improvement would need to combine the structural analysis this report is concerned about with semantic analysis.

\subsection{Automatic Link Insertion}
Once the system is able to suggest precise link locations, an improvement would be to automatically insert suggested links in articles once they are marked by users to be appropriate. This would significantly would decrease the workload required by editors.

\subsection{Article Categories}
The current graph model of Wikipedia just models links between articles. We can extend this model to also include categorical information on articles, hereby adding additional relatedness knowledge to the graph. The feature learning component could then both learn features on the links sub graph and the categories sub graph, hopefully leading to better classification results.