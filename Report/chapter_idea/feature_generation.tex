\section{Feature Generation}
A main component of any machine learning technique is features. The features should be used to discriminate the data so that a wanted structure appears. Deriving good features from a data set is paramount to the performance of the machine learning algorithm.

Traditionally, features are engineered from a large data set. This means that particular parts of the data set that are deemed characteristic for a given problem are hand-picked by humans. In recent years, feature learning has gained popularity. In short, features are automatically generated by looking at the structure of a data set.

In the following sections we will discuss strengths and weaknesses of each approach to feature generation and how they are used in our project.

\subsection{Feature Engineering}
The premise of feature engineering is that humans are able to identify information in a data set that has a high degree of discrimination power. For example, in a binary classification problem, this means features that are good at separating positive and negative samples. Identifying these features may require domain knowledge as the best features may not be the obvious ones. In general it is a time consuming process to identify suitable features. Typically, many trial and error iterations are needed to derive good features.

We do not have great domain knowledge of Wikipedia, but we have some ideas for features that intuitively should be good.

\begin{description}
    \item[Clickstream] \todo{describe what this is before? link to wikipedia clickstream and maybe alexanders regression paper}.
    \item[Terms similarity] This is a metric that describes the similarity of the most important terms on two given Wikipedia articles. For example, if article A has the terms w1, w2, w3 and article B has w3, w4, a metric could be the number of similar terms, in this case 1. The idea behind the feature is that articles with similar terms are more likely to be linked together.
    \item[Shortest path] Consider Wikipedia as a graph, where the vertices are articles and edges are links between articles. The idea of this feature is that there is a correlation between the shortest path of two articles A, B, and whether there is a link between A and B. 
    \item[Common children/parents] See figure \todo{lav figure om children/parents, sp√∏rg michael}. The motivation of this feature is that if two articles have common children or common parents, they may be related to such a degree that they should be linked.
\end{description}


\begin{figure}[tbp]%
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{parents}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]10:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]150:}
              ] (p) {P};           
      \node [node, below left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:}, 
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]100:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]200:}
              ] (a) {A};           
      \node [node, below right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:}, 
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]40:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]85:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]200:}
              ] (b) {B};
      \path [line, very thick] (p) -- (a);
      \path [line, very thick] (p) -- (b);
    \end{tikzpicture}
    \caption[short desc]{A nice figure showing a common direct predecessor of A and B}%
    \label{fig:parent-rel}%
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \tikzsetnextfilename{children}
    \begin{tikzpicture}[node distance = 1.7cm, auto]
      \node [node,
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]350:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]210:}
              ] (p) {S};           
      \node [node, above left=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]15:}, 
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]230:},
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]280:}
              ] (a) {A};           
      \node [node, above right=of p,
              pin={[pin distance=1.2em,pin edge={stealth-, thin}]330:}, 
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]280:},
              pin={[pin distance=1.2em,pin edge={-stealth, thin}]150:}
              ] (b) {B};
      \path [line, very thick] (a) -- (p);
      \path [line, very thick] (b) -- (p);
    \end{tikzpicture}
    \caption[short desc]{A very nice figure showing a common direct successor of A and B}%
    \label{fig:children-rel}%
  \end{minipage}
  

\end{figure}

\subsection{Feature Learning}
The obvious advantage of feature learning is that it obviates the need for feature engineering. Instead of manually deriving discriminating information, features are generated by the structure of the data set. This has the advantage that the need for domain knowledge is reduced, and the feature learning algorithm is able to uncover structural patterns that are hard for humans to discover. As the features are generated, they are difficult to reason about. In other words, there is limited intuitive understanding available that can explain what each individual feature means. As the feature learning process is mostly autonomous, it is possible to mechanically try many combinations of parameters for the algorithm, in order to find the best set of features.


\todo{Missing: Our choice/idea/solution/guess}