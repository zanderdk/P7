\chapter{Test \& Evaluation}\label{chap:testeval}
This chapter is evaluates the performance of our proposed solution. We will describe the methodology of the tests and interpret the results.

\section{Classification}
This section will describe the testing of the entire system by interpreting the results of the classifier.

The performance of the classifier model is evaluated on a test dataset that has not been part of the training data. The result is derived from the same evaluation metric as used in~\cref{evaluation_metric}. The test data consists of 20\% of all data samples. This amounts to \num{294858} samples, evenly split between positive and negative samples as defined in \cref{eq:training_pairs}. Based on the evaluation of different classifiers in~\cref{choosing_classifier}, we have chosen the \emph{nearest centroid} implementation from scikit-learn with its default parameters.

\subsection{Results and Evaluation}
The nearest centroid classifier has a precision of \num{0.981} and a recall of \num{0.52}. The results are similar to the results from the training phase. The precision is very high, while the recall is low. On the specific dataset (links from featured articles) we test on, the classifier is very accurate at predicting whether or not a pair of articles should be linked. The confusion matrix in \cref{tab:confusionmatrix} shows that 26\% of all samples are predicted to be positive, which is the proportion of classified links that are suggested to the user.  Of all the predicted positives, \num{1462} (2\%) are misclassified. \num{70755} (33\%) of the negative predictions are actual positives that was incorrectly classified, causing them to be excluded as possible suggestions. The low number of false positive misclassifications and the relatively higher number of false negative misclassifications are due to optimizing for precision over recall.  \num{78136} (26\%) suggestions are more than enough for a low number of editors to review, but scaling the number of users will ultimately lead to exhaustion of all suggestions. We can not reasonably say at this point whether it was a good idea to optimize solely for precision. We might gain the \num{70755} suggestions that were false negatives by also considering recall. Whether these are needed will therefore have to be reconsidered if the system is put into use.

Suspicious of the high precision rate, we test the classifier on articles not in the featured article space by manually labeling a small number of articles. These tests indicate poor performance in predictions, which indicates that the classifier results do not generalize to the whole of Wikipedia, i.e.\ to articles that are not featured. In hindsight, this can be expected as the structural difference between non-featured articles and featured articles likely is large.

%We can conclude that the performance of the system was good on featured articles, but the results do not transfer well to other articles. This can be explained because the traning data only consist of links from featured articles. 
%The traning data only consists of 0.21\% of all the links on Wikipedia, it could be that we are traning on a to small set of pairs and as a consequence of that we are overfitting to this sub set. The reason why we are preforming good at the test data could then be that the test data is constructed from featured articles as well as the traning data and there by the two data sets are to similar. Another reason could be that randomly picking actiles for sampling negative traning pairs, resulted in having to few related pairs that should not be linked. 

The performance of the system is satisfactory on featured articles, which is also the data on which we trained the classifier. We see that these results do not transfer as well to all articles as we would have liked.


%\begin{table}[htbp]
%\centering
%\begin{tabular}{@{}ll@{}}
%\toprule
%Precision     & Recall       \\ \midrule
%\num{0.981} & \num{0.52} \\
%\bottomrule
%\end{tabular}
%\caption[Nearest Centroid classifier results]{Nearest Centroid classifier results}\label{eval-results}
%\end{table}

\begin{table}[tbp]
    \centering
     \begin{tabular}{rrrrrrrr}
      \toprule
      \multicolumn{2}{c}{} & \multicolumn{2}{c}{Predicted} & \\
      \cmidrule{3-4}
      \multicolumn{2}{c}{} & \multicolumn{1}{c}{Positive} & \multicolumn{1}{c}{Negative} & Total \\
      \midrule
      \multirow{2}{*}{Actual} & Positive & \num{76674} (0.98) & \num{70755} (0.33)  & \num{147429} \\
                              & Negative & \num{1462} (0.02)  & \num{145967} (0.67) & \num{147429} \\
                              \cmidrule{1-2}
                              & Total    & \num{78136} (0.26) & \num{216722} (0.74) \\
      \bottomrule
    \end{tabular}
    \caption[Confusion Matrix]{Confusion Matrix}%
    \label{tab:confusionmatrix}
\end{table}

\section{User Interface}
This section will describe the test and evaluation of the implementation of the user interface described in \cref{sec:design_ui}.

\subsection{Methodology}
Our user interface is presented as an API, and a number of methods exists to test these~\cite{swebok}. Because of limitations, we have not refined the UI in areas such as load balancing, security, performance, and installation, and as such we will not test these areas. Instead we will focus on interface testing and acceptance testing.

Interface testing simulates the usage of an API, and checks if it adheres to some requirement~\cite{swebok}. To perform this testing, we will utilize a mix of unit testing and system testing, in order to, respectively, assess the behavior of the API code itself, and to assess the external interface to other applications. The test technique will primarily be error-guessing~\cite{swebok}, where you as a programmer try to anticipate where the most plausible faults may occur.

Acceptance testing determines if a system satisfies an acceptance criteria, by checking against the customer's requirements. As we have no customers, we instead came up with our own requirements in \cref{sec:uireqs}.

%To test the UI we define unit tests that each test an isolated part of the whole. By splitting the test into smaller parts, it is easier to reason about the results of each part. The combination of all unit tests should be able to confidently verify that the implementation is working as intended.

Before testing, we also formalize which areas we will test. The UI consists of two HTTP endpoints, GET link suggestions and POST review, which each will need testing. The structure of each endpoint testing procedure is similar in that we make a HTTP request and make assertions on the response.

\subsection{Interface Testing}
In order to test whether the API adheres to our requirements, we have written a test suite of unit testing and system testing components. This will constitute our interface testing. We check if the responses are well-formed, i.e.\ returning the correct response codes and the correct JSON elements. These tests include testing for empty responses, correctly formed JSON code, and if the response is appropriate for different input arguments.

All tests we have written passes. This means we can with some level of confidence say that our API works under normal use cases. However, as stated before, we do not consider for example security or stress testing, and as such we cannot say with any confidence that the system will work under load or in the presence of malicious users.

\subsection{Acceptance Testing}
In \cref{sec:uireqs} we found the two requirements:
\begin{itemize}
	\item A user must be able to query the UI for link suggestions
	\item A user should be able to submit reviews of link suggestions
\end{itemize}

Based on the interface testing, we can also say that the API fulfills the first requirement. However, we have not implemented the second requirement into the main pipeline, and as such this assessment has to be taken with a grain of salt. We cannot fully test whether a review is saved and the system sufficiently takes it into account. Therefore, the acceptance test is, as expected, only partially successful. \todo{Hvor er det nu vi har fraskrevet os at implementere dette? Det kunne v√¶re nice at henvise til.}
