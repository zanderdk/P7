\chapter{Test \& Evaluation}\label{chap:testeval}
This chapter is evaluates the performance of our proposed solution. We will describe the methodology of the tests and interpret the results.

\section{Classification}
This section will describe the testing of the entire system by interpreting the results of the classifier.

The performance of the classifier model is evaluated on a test dataset that has not been part of the training data. The result is derived from the same evaluation metric as used in~\cref{evaluation_metric}. The test data consists of 20\% of all data samples. This amounts to \num{294858} samples, evenly split between positive and negative samples as defined in \cref{eq:training_pairs}. Based on the evaluation of different classifiers in~\cref{choosing_classifier}, we have chosen the \emph{nearest centroid} implementation from scikit-learn with its default parameters.

\subsection{Results and Evaluation}
The nearest centroid classifier has a precision of \num{0.981} and a recall of \num{0.52}. The results are similar to the results from the training phase. The precision is very high, while the recall is low. On the specific dataset (links from featured articles) we test on, the classifier is very accurate at predicting whether or not a pair of articles should be linked. The confusion matrix in \cref{tab:confusionmatrix} shows that 26\% of all samples are predicted to be positive, which is the proportion of classified links that are suggested to the user.  Of all the predicted positives, \num{1462} (2\%) are misclassified. \num{70755} (33\%) of the negative predictions are actual positives that was incorrectly classified, causing them to be excluded as possible suggestions. The low number of false positive misclassifications and the relatively higher number of false negative misclassifications are due to optimizing for precision over recall.  \num{78136} (26\%) suggestions are more than enough for a low number of editors to review, but scaling the number of users will ultimately lead to exhaustion of all suggestions. We can not reasonably say at this point whether it was a good idea to optimize solely for precision. We might gain the \num{70755} suggestions that were false negatives by also considering recall. Whether these are needed will therefore have to be reconsidered if the system is put into use.

Suspicious of the high precision rate, we test the classifier on articles not in the featured article space by manually labeling a small number of articles. These tests indicate poor performance in predictions, which indicates that the classifier results do not generalize to the whole of Wikipedia, i.e.\ to articles that are not featured. In hindsight, this can be expected as the structural difference between non-featured articles and featured articles likely is large.

%We can conclude that the performance of the system was good on featured articles, but the results do not transfer well to other articles. This can be explained because the traning data only consist of links from featured articles. 
%The traning data only consists of 0.21\% of all the links on Wikipedia, it could be that we are traning on a to small set of pairs and as a consequence of that we are overfitting to this sub set. The reason why we are preforming good at the test data could then be that the test data is constructed from featured articles as well as the traning data and there by the two data sets are to similar. Another reason could be that randomly picking actiles for sampling negative traning pairs, resulted in having to few related pairs that should not be linked. 

The performance of the system is satisfactory on featured articles, which is also the data on which we trained the classifier. We see that these results do not transfer as well to all articles as we would have liked.


%\begin{table}[htbp]
%\centering
%\begin{tabular}{@{}ll@{}}
%\toprule
%Precision     & Recall       \\ \midrule
%\num{0.981} & \num{0.52} \\
%\bottomrule
%\end{tabular}
%\caption[Nearest Centroid classifier results]{Nearest Centroid classifier results}\label{eval-results}
%\end{table}

\begin{table}[tbp]
    \centering
     \begin{tabular}{rrrrrrrr}
      \toprule
      \multicolumn{2}{c}{} & \multicolumn{2}{c}{Predicted} & \\
      \cmidrule{3-4}
      \multicolumn{2}{c}{} & \multicolumn{1}{c}{Positive} & \multicolumn{1}{c}{Negative} & Total \\
      \midrule
      \multirow{2}{*}{Actual} & Positive & \num{76674} (0.98) & \num{70755} (0.33)  & \num{147429} \\
                              & Negative & \num{1462} (0.02)  & \num{145967} (0.67) & \num{147429} \\
                              \cmidrule{1-2}
                              & Total    & \num{78136} (0.26) & \num{216722} (0.74) \\
      \bottomrule
    \end{tabular}
    \caption[Confusion Matrix]{Confusion Matrix}%
    \label{tab:confusionmatrix}
\end{table}

\section{User Interface}
This section will describe the systematic test and evaluation of the implementation of the user interface described in \cref{sec:design_ui}.

\subsection{Methodology}
To test the UI we define unit tests that each test an isolated part of the whole. By splitting the test into smaller parts, it is easier to reason about the results of each part. The combination of all unit tests should be able to confidently verify that the implementation is working as intended.

The UI consists of two HTTP endpoints which we will test: GET link suggestions and POST review. The structure of each unit test is similar; make a HTTP request and make assertions on the response. 

The test is considered a success if all unit tests succeed.

\todo{Omskriv hele dette afsnit}

\subsection{Results}
All tests pass, which indicates that the user interface satisfies the requirements proposed. \todo{meget kort, hvordan kan det udvides?} \todo{kan vi liste hvad der blev testet, vores requirements?}
