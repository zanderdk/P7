\chapter{Test \& Evaluation}\label{chap:testeval}
This chapter is dedicated to evaluate the performance of our proposed solution. We will describe the methodology of the tests, and interpret the results.

\section{Methodology}
We test the classifier model by predicting links on data samples that has not been part of the training data, and deriving a result based on the same evaluation metric as in \cref{evaluation_metric}: precision.

As described in \cref{sec:training_data} the test data consists of 20\% of all data samples. This accounts to \num{294858} samples, evenly split between positives and negatives. Based on the classifier tests performed in \cref{choosing_classifier}, we use the \emph{nearest centroid} implementation from sci-kit learn with its default parameters.

\section{Evaluation}
The classifier results can be seen in \cref{eval-results}. The numbers are similar to the ones from the training phase. The precision is very good, while the recall is low. On the specific data set we have worked on, links from featured articles, the classifier is very good at predicting links. Suspicious of the high precision numbers, we tested the classifier on articles not in the featured article space by manually labeling some articles. These tests show poor predictions, which means that the classifier results do not generalize to the whole of Wikipedia. This is to be expected, as one would assume that the structural differences between non-featured articles and featured articles are large.

We can conclude that the performance of the system was good on featured articles, but the results do not transfer well to all articles.

\begin{table}[htbp]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Precision     & Recall       \\ \midrule
\num{0.981} & \num{0.52}
\end{tabular}
\caption{Classifier results}\label{eval-results}
\end{table}

\begin{table}[tbp]
    \centering
     \begin{tabular}{rrrrrrrr}
      \toprule
      \multicolumn{2}{c}{} & \multicolumn{2}{c}{Predicted} & \\
      \cmidrule{3-4}
      \multicolumn{2}{c}{} & \multicolumn{1}{c}{Positive} & \multicolumn{1}{c}{Negative} & Total \\
      \midrule
      \multirow{2}{*}{Actual} & Positive & \num{76674} & \num{70755}  & \num{147429} \\
                              & Negative & \num{1462}  & \num{145967} & \num{147429} \\ \cmidrule{1-2}
                              & Total    & \num{78136} & \num{216722} \\
      \bottomrule
    \end{tabular}
    \caption[Confusion Matrix]{Confusion Matrix}%
    \label{tab:confusionmatrix}
\end{table}

\section{GUI Evaluation}