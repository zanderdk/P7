\chapter{Test \& Evaluation}\label{chap:testeval}
This chapter evaluates the performance of our proposed solution. We describe the methodology of the tests and interpret the results.

\section{Classification}\label{sec:classification_evaluation}

The performance of the classifier is evaluated on a test dataset that has not been part of the training data. The result is derived from the same evaluation metric as used in~\cref{evaluation_metric}. As described in \cref{sec:training_data} the test data consists of 20\% of all data samples. This amounts to \num{294858} samples, split evenly between positive and negative samples. Based on the evaluation of different classifiers in~\cref{choosing_classifier}, we have chosen the \emph{nearest centroid} implementation from scikit-learn.

\subsection{Results and Evaluation}

As shown in \cref{eval-results}, the result of running the nearest centroid classifier on the test data gave a precision of \num{0.981} and a recall of \num{0.52}. The results are similar to the results from the training phase. The precision is very high, while the recall is low. On the specific dataset we test on, the classifier is very accurate at predicting if a pair of articles should be linked. The confusion matrix in \cref{tab:confusionmatrix} shows that 26\% of all samples are predicted to be positive, which is the proportion of classified links that are suggested to the user. Of all the predicted positives, \num{1462} (2\%) are misclassified. \num{70755} (33\%) of the negative predictions are actual positives that was incorrectly classified, causing them to be excluded as possible suggestions. The low number of false positives and the relatively high number of false negatives are due to optimizing for precision over recall. 

\num{78136} (26\%) suggestions are sufficient for a low number of editors to review, but scaling the number of users will ultimately lead to exhaustion of all suggestions. If we focused more on recall, we could reduce the number of false negatives, increasing the number of link suggestions. Whether this is necessary would have to be reevaluated when the system is in use.

\begin{table}[htbp]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Precision     & Recall       \\ \midrule
\num{0.981} & \num{0.52} \\
\bottomrule
\end{tabular}
\caption[Nearest Centroid classifier results]{\emph{Nearest Centroid} classifier results}%
\label{eval-results}
\end{table}

Suspicious of the high precision rate, we tested the classifier on articles not in the featured article space by manually labeling a small number of articles. The test showed poor performance in predictions, indicating that the classifier does not generalize to the whole of Wikipedia, i.e.\ to articles that are not featured.

After performing these manual tests we compared the feature vectors of a few different articles. From this we noticed a tendency of high cosine similarity between well-linked articles, and likewise between articles with no links, regardless of subject relevance. It might be the case that the random generation of training data, was influenced by the existence of many non-encyclopedia articles, causing the training and test data to not be representative of the cases we want to classify. It could also be that many of the biased random walks ends prematurely because they reach articles with no outgoing links, which might negatively affect the feature learning model.


%\begin{table}[htbp]
%\centering
%\begin{tabular}{@{}ll@{}}
%\toprule
%Precision     & Recall       \\ \midrule
%\num{0.981} & \num{0.52} \\
%\bottomrule
%\end{tabular}
%\caption[Nearest Centroid classifier results]{Nearest Centroid classifier results}\label{eval-results}
%\end{table}

\begin{table}[tbp]
    \centering
     \begin{tabular}{rrrrrrrr}
      \toprule
      \multicolumn{2}{c}{} & \multicolumn{2}{c}{Predicted} & \\
      \cmidrule{3-4}
      \multicolumn{2}{c}{} & \multicolumn{1}{c}{Positive} & \multicolumn{1}{c}{Negative} & Total \\
      \midrule
      \multirow{2}{*}{Actual} & Positive & \num{76674} (0.98) & \num{70755} (0.33)  & \num{147429} \\
                              & Negative & \num{1462} (0.02)  & \num{145967} (0.67) & \num{147429} \\
                              \cmidrule{1-2}
                              & Total    & \num{78136} (0.26) & \num{216722} (0.74) \\
      \bottomrule
    \end{tabular}
    \caption[Confusion Matrix]{Confusion Matrix showing the results of the \emph{Nearest Centroid} classifier test.}%
    \label{tab:confusionmatrix}
\end{table}

\section{User Interface}
Our user interface is presented as an API, and a number of methods exists to test these~\cite{swebok}. Because of the delimitations made in \cref{sec:design_ui}, we have not refined the UI in areas such as load balancing, security, and performance, and as such we will not test these areas. Instead we will focus on interface testing and acceptance testing.

Interface testing simulates the usage of an API, and checks if it adheres to some requirement~\cite{swebok}. We perform a combination of unit testing and system testing, in order to assess the behavior of the API code itself, and the external interface to other applications. The test technique is primarily error-guessing~\cite{swebok}, where we anticipate where the most plausible faults may occur.

Acceptance testing determines if a system satisfies an acceptance criteria, by checking against the customer's requirements. As we have no customers, we instead proposed our own requirements in \cref{sec:uireqs}.

%To test the UI we define unit tests that each test an isolated part of the whole. By splitting the test into smaller parts, it is easier to reason about the results of each part. The combination of all unit tests should be able to confidently verify that the implementation is working as intended.

The UI consists of two HTTP endpoints, \emph{Query for link(s)} and \emph{Review report}, which are tested. Both test procedures send HTTP requests and make assertions on the responses.

\subsection{Interface Testing}
In order to test whether the API adheres to our requirements, we have written a test suite of unit testing and system testing components. This will constitute our interface testing. We check if the responses are well-formed, i.e.\ returning the correct response codes and the correct JSON elements. These tests include testing for empty responses, correctly formed JSON code, and if the response is appropriate for different input arguments.

All tests we have written passes, indicating that our API works under normal use cases. However, as stated before, we do not consider for example security or stress testing, and as such we cannot say with any confidence that the system will work under load, or in the presence of malicious users.

\subsection{Acceptance Testing}
In \cref{sec:uireqs} we found the two requirements:
\begin{itemize}
	\item A user must be able to query the UI for link suggestions
	\item A user should be able to submit reviews of link suggestions
\end{itemize}

Based on the interface testing, we can say that the API fulfills the first requirement. Even though we have not implemented the functionality in the main pipeline that could benefit from the second UI requirement, the requirement is still fulfilled. Therefore we conclude the acceptance test as passed.

\todo{er der ikke et swebok navn for acceptence test der kun er for den forel√∏bige kravpspecifikation}