\chapter{Test \& Evaluation}\label{chap:testeval}
This chapter is dedicated to evaluate the performance of our proposed solution. We will describe the methodology of the tests, and interpret the results.

\section{Methodology}
We test the classifier model by predicting links on data samples that has not been part of the training data, and deriving a result based on the same evaluation metric as in \cref{evaluation_metric}: precision.

As described in \cref{sec:training_data} the test data consists of 20\% of all data samples. This amounts to \num{294858} samples, evenly split between positives and negatives. Based on the classifier tests performed in \cref{choosing_classifier}, we use the \emph{nearest centroid} implementation from sci-kit learn with its default parameters.

\section{Evaluation}
The classifier results can be seen in \cref{eval-results}. The numbers are similar to the ones from the training phase. The precision is very good, while the recall is low. On the specific data set we have worked on, links from featured articles, the classifier is very good at predicting links. The confusion matrix in \cref{tab:confusionmatrix} shows that 27\% of all samples are predicted to be positive, while 74\% are predicted negative. This is to be expected, as we optimized for precision. Of all the predicted positives, 1462 (2\%) were misclassified. \num{70755} (33\%) were incorrectly classified as negative, meaning we missed out on suggestion them. Again, a trade-off of optimizing for precision over recall.

Suspicious of the high precision numbers, we tested the classifier on articles not in the featured article space by manually labeling some articles. These tests show poor predictions, which indicates that the classifier results do not generalize to the whole of Wikipedia. This is to be expected, as one would assume that the structural differences between non-featured articles and featured articles are large.

We can conclude that the performance of the system was good on featured articles, but the results do not transfer well to other articles. This can be explained because the traning data only consist of links from featured articles. 
The traning data only consists of 0.21\% of all the links on Wikipedia, it could be that we are traning on a to small set of pairs and as a consequence of that we are overfitting to this sub set. The reason why we are preforming good at the test data could then be that the test data is constructed from featured articles as well as the traning data and there by the two data sets are to similar. Another reason could be that randomly picking actiles for sampling negative traning pairs, resulted in having to few related pairs that should not be linked. 

\begin{table}[htbp]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Precision     & Recall       \\ \midrule
\num{0.981} & \num{0.52} \\
\bottomrule
\end{tabular}
\caption[Nearest Centroid classifier results]{Nearest Centroid classifier results}\label{eval-results}
\end{table}

\begin{table}[htbp]
    \centering
     \begin{tabular}{rrrrrrrr}
      \toprule
      \multicolumn{2}{c}{} & \multicolumn{2}{c}{Predicted} & \\
      \cmidrule{3-4}
      \multicolumn{2}{c}{} & \multicolumn{1}{c}{Positive} & \multicolumn{1}{c}{Negative} & Total \\
      \midrule
      \multirow{2}{*}{Actual} & Positive & \num{76674} (0.98) & \num{70755} (0.33)  & \num{147429} \\
                              & Negative & \num{1462} (0.02)  & \num{145967} (0.67) & \num{147429} \\
                              \cmidrule{1-2}
                              & Total    & \num{78136} (0.26) & \num{216722} (0.74) \\
      \bottomrule
    \end{tabular}
    \caption[Confusion Matrix]{Confusion Matrix}%
    \label{tab:confusionmatrix}
\end{table}



%\section{UI Evaluation}
